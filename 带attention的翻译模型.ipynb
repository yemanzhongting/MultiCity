{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "带attention的翻译模型.ipynb",
      "provenance": [],
      "mount_file_id": "1FiXDCPSMhS70srtdO42U-HEujm_liSRX",
      "authorship_tag": "ABX9TyO2myP0u9OG0EV03NjjkRJr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yemanzhongting/MultiCity/blob/main/%E5%B8%A6attention%E7%9A%84%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hliC0pKsCCQ4"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import nltk"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3UD1PxfCF_5"
      },
      "source": [
        "def load_data(in_file):\n",
        "    cn = []\n",
        "    en = []\n",
        "    num_examples = 0\n",
        "    with open(in_file, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip().split(\"\\t\")\n",
        "            \n",
        "            en.append([\"BOS\"] + nltk.word_tokenize(line[0].lower()) + [\"EOS\"])\n",
        "            # split chinese sentence into characters\n",
        "            cn.append([\"BOS\"] + [c for c in line[1].split(' ')] + [\"EOS\"])\n",
        "    return en, cn"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwjJkPNUCIcd",
        "outputId": "06c5a140-63bf-40dc-fbe3-c455138e7c11"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "train_file = \"/content/drive/MyDrive/data/sv-sub.txt\"\n",
        "dev_file = \"nmt/en-cn/dev.txt\"\n",
        "train_en, train_cn = load_data(train_file)\n",
        "# dev_en, dev_cn = load_data(dev_file)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBXM2KemGulM",
        "outputId": "c76b9752-59d7-4fee-a493-91e205ed88f8"
      },
      "source": [
        "len(train_en[0])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbM0Eih0F5t1"
      },
      "source": [
        "# dev_en"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6Al_6KlFLCI"
      },
      "source": [
        "dev_en=train_en[0:300]\n",
        "dev_cn=train_cn[0:300]"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnEnT2SVC-5q"
      },
      "source": [
        "https://github.com/ZeweiChu/nmt-seq2seq/blob/master/pytorch/seq2seq.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPIiW_rAC6by"
      },
      "source": [
        "UNK_IDX = 0\n",
        "PAD_IDX = 1\n",
        "def build_dict(sentences, max_words=50000):\n",
        "    word_count = Counter()\n",
        "    for sentence in sentences:\n",
        "        for s in sentence:\n",
        "            word_count[s] += 1\n",
        "    ls = word_count.most_common(max_words)\n",
        "    total_words = len(ls) + 2\n",
        "    word_dict = {w[0]: index+2 for index, w in enumerate(ls)}\n",
        "    word_dict[\"UNK\"] = UNK_IDX\n",
        "    word_dict[\"PAD\"] = PAD_IDX\n",
        "    return word_dict, total_words\n",
        "\n",
        "en_dict, en_total_words = build_dict(train_en)\n",
        "cn_dict, cn_total_words = build_dict(train_cn)\n",
        "inv_en_dict = {v: k for k, v in en_dict.items()}\n",
        "inv_cn_dict = {v: k for k, v in cn_dict.items()}"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3KUBgOEIDUc",
        "outputId": "70f82f7e-ee41-46ad-b025-126010f974b3"
      },
      "source": [
        "len(inv_en_dict)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "143"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gK9_pbTDA7m",
        "outputId": "a0abbcf6-ea8a-473e-af8e-10de6f833f13"
      },
      "source": [
        "len(inv_cn_dict)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "158"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvkFeYpODEkU"
      },
      "source": [
        "def encode(en_sentences, cn_sentences, en_dict, cn_dict, sort_by_len=True):\n",
        "    '''\n",
        "        Encode the sequences. \n",
        "    '''\n",
        "    length = len(en_sentences)\n",
        "    out_en_sentences = [[en_dict.get(w, 0) for w in sent] for sent in en_sentences]\n",
        "    out_cn_sentences = [[cn_dict.get(w, 0) for w in sent] for sent in cn_sentences]\n",
        "\n",
        "    # sort sentences by english lengths\n",
        "    def len_argsort(seq):\n",
        "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
        "       \n",
        "    if sort_by_len:\n",
        "        sorted_index = len_argsort(out_en_sentences)\n",
        "        out_en_sentences = [out_en_sentences[i] for i in sorted_index]\n",
        "        out_cn_sentences = [out_cn_sentences[i] for i in sorted_index]\n",
        "        \n",
        "    return out_en_sentences, out_cn_sentences\n",
        "\n",
        "train_en, train_cn = encode(train_en, train_cn, en_dict, cn_dict)\n",
        "# dev_en, dev_cn = encode(dev_en, dev_cn, en_dict, cn_dict)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4eNXOqnG-pz"
      },
      "source": [
        "dev_en, dev_cn = encode(dev_en, dev_cn, en_dict, cn_dict)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yknReJ6lDImg"
      },
      "source": [
        "def get_minibatches(n, minibatch_size, shuffle=True):\n",
        "    idx_list = np.arange(0, n, minibatch_size)\n",
        "    if shuffle:\n",
        "        np.random.shuffle(idx_list)\n",
        "    minibatches = []\n",
        "    for idx in idx_list:\n",
        "        minibatches.append(np.arange(idx, min(idx + minibatch_size, n)))\n",
        "    return minibatches\n",
        "\n",
        "def prepare_data(seqs):\n",
        "    lengths = [len(seq) for seq in seqs]\n",
        "    n_samples = len(seqs)\n",
        "    max_len = np.max(lengths)\n",
        "\n",
        "    x = np.zeros((n_samples, max_len)).astype('int32')\n",
        "    x_lengths = np.array(lengths).astype(\"int32\")\n",
        "    for idx, seq in enumerate(seqs):\n",
        "        x[idx, :lengths[idx]] = seq\n",
        "    return x, x_lengths #x_mask\n",
        "\n",
        "def gen_examples(en_sentences, cn_sentences, batch_size):\n",
        "    minibatches = get_minibatches(len(en_sentences), batch_size)\n",
        "    all_ex = []\n",
        "    for minibatch in minibatches:\n",
        "        mb_en_sentences = [en_sentences[t] for t in minibatch]\n",
        "        mb_cn_sentences = [cn_sentences[t] for t in minibatch]\n",
        "        mb_x, mb_x_len = prepare_data(mb_en_sentences)\n",
        "        mb_y, mb_y_len = prepare_data(mb_cn_sentences)\n",
        "        all_ex.append((mb_x, mb_x_len, mb_y, mb_y_len))\n",
        "    return all_ex\n",
        "\n",
        "batch_size = 64\n",
        "train_data = gen_examples(train_en, train_cn, batch_size)\n",
        "# random.shuffle(train_data)\n",
        "dev_data = gen_examples(train_en[0:300], train_cn[0:300], batch_size)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPrtVpSJEZgA",
        "outputId": "fc52e272-5778-4aa5-905c-d785ef6b610d"
      },
      "source": [
        "batch_size"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5ZdZFUHDL2c"
      },
      "source": [
        "# train_data"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdiu63SZDPd9"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = nn.GRU(embed_size, enc_hidden_size, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(enc_hidden_size * 2, dec_hidden_size)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        sorted_len, sorted_idx = lengths.sort(0, descending=True)\n",
        "        x_sorted = x[sorted_idx.long()]\n",
        "        embedded = self.dropout(self.embed(x_sorted))\n",
        "        \n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
        "        packed_out, hid = self.rnn(packed_embedded)\n",
        "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
        "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
        "        out = out[original_idx.long()].contiguous()\n",
        "        hid = hid[:, original_idx.long()].contiguous()\n",
        "        \n",
        "        hid = torch.cat([hid[-2], hid[-1]], dim=1)\n",
        "        hid = torch.tanh(self.fc(hid)).unsqueeze(0)\n",
        "\n",
        "        return out, hid"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcyr6HH9DS_U"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hidden_size, dec_hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.enc_hidden_size = enc_hidden_size\n",
        "        self.dec_hidden_size = dec_hidden_size\n",
        "\n",
        "        self.linear_in = nn.Linear(enc_hidden_size*2, dec_hidden_size, bias=False)\n",
        "        self.linear_out = nn.Linear(enc_hidden_size*2 + dec_hidden_size, dec_hidden_size)\n",
        "        \n",
        "    def forward(self, output, context, mask):\n",
        "        # output: batch_size, output_len, dec_hidden_size\n",
        "        # context: batch_size, context_len, enc_hidden_size\n",
        "    \n",
        "        batch_size = output.size(0)\n",
        "        output_len = output.size(1)\n",
        "        input_len = context.size(1)\n",
        "        \n",
        "        context_in = self.linear_in(context.view(batch_size*input_len, -1)).view(                batch_size, input_len, -1) # batch_size, output_len, dec_hidden_size\n",
        "        attn = torch.bmm(output, context_in.transpose(1,2)) # batch_size, output_len, context_len\n",
        "\n",
        "        \n",
        "        attn.data.masked_fill(mask, -1e6)\n",
        "\n",
        "        attn = F.softmax(attn, dim=2) # batch_size, output_len, context_len\n",
        "\n",
        "        context = torch.bmm(attn, context) # batch_size, output_len, enc_hidden_size\n",
        "        \n",
        "        output = torch.cat((context, output), dim=2) # batch_size, output_len, hidden_size*2\n",
        "\n",
        "        \n",
        "        output = output.view(batch_size*output_len, -1)\n",
        "        output = torch.tanh(self.linear_out(output))\n",
        "        output = output.view(batch_size, output_len, -1)\n",
        "        return output, attn"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rhY-7FoDZic"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.attention = Attention(enc_hidden_size, dec_hidden_size)\n",
        "        self.rnn = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(dec_hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def create_mask(self, x_len, y_len):\n",
        "        device = x_len.device\n",
        "        max_x_len = x_len.max()\n",
        "        max_y_len = y_len.max()\n",
        "        x_mask = torch.arange(max_x_len, device=x_len.device)[None, :] < x_len[:, None]\n",
        "        y_mask = torch.arange(max_y_len, device=x_len.device)[None, :] < y_len[:, None]\n",
        "        mask = (~x_mask[:, :, None] * y_mask[:, None, :]).byte()\n",
        "        return mask\n",
        "        \n",
        "        \n",
        "    def forward(self, ctx, ctx_lengths, y, y_lengths, hid):\n",
        "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n",
        "        y_sorted = y[sorted_idx.long()]\n",
        "        hid = hid[:, sorted_idx.long()]\n",
        "        \n",
        "        y_sorted = self.dropout(self.embed(y_sorted)) # batch_size, output_length, embed_size\n",
        "\n",
        "        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
        "        out, hid = self.rnn(packed_seq, hid)\n",
        "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
        "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
        "        output_seq = unpacked[original_idx.long()].contiguous()\n",
        "        hid = hid[:, original_idx.long()].contiguous()\n",
        "\n",
        "        mask = self.create_mask(y_lengths, ctx_lengths)\n",
        "\n",
        "        # code.interact(local=locals())\n",
        "        output, attn = self.attention(output_seq, ctx, mask)\n",
        "        output = F.log_softmax(self.out(output), -1)\n",
        "        \n",
        "        return output, hid, attn"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtOkVe7YDbHa"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        \n",
        "    def forward(self, x, x_lengths, y, y_lengths):\n",
        "        encoder_out, hid = self.encoder(x, x_lengths)\n",
        "        output, hid, attn = self.decoder(ctx=encoder_out, \n",
        "                    ctx_lengths=x_lengths,\n",
        "                    y=y,\n",
        "                    y_lengths=y_lengths,\n",
        "                    hid=hid)\n",
        "        return output, attn\n",
        "    \n",
        "    def translate(self, x, x_lengths, y, max_length=100):\n",
        "        encoder_out, hid = self.encoder(x, x_lengths)\n",
        "        preds = []\n",
        "        batch_size = x.shape[0]\n",
        "        attns = []\n",
        "        for i in range(max_length):\n",
        "            output, hid, attn = self.decoder(ctx=encoder_out, \n",
        "                    ctx_lengths=x_lengths,\n",
        "                    y=y,\n",
        "                    y_lengths=torch.ones(batch_size).long().to(y.device),\n",
        "                    hid=hid)\n",
        "            y = output.max(2)[1].view(batch_size, 1)\n",
        "            preds.append(y)\n",
        "            attns.append(attn)\n",
        "        return torch.cat(preds, 1), torch.cat(attns, 1)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohZ4QemmDda7"
      },
      "source": [
        "class LanguageModelCriterion(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LanguageModelCriterion, self).__init__()\n",
        "\n",
        "    def forward(self, input, target, mask):\n",
        "        input = input.contiguous().view(-1, input.size(2))\n",
        "        target = target.contiguous().view(-1, 1)\n",
        "        mask = mask.contiguous().view(-1, 1)\n",
        "        output = -input.gather(1, target) * mask\n",
        "        output = torch.sum(output) / torch.sum(mask)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qftBAZ5BDerk"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "en_vocab_size = len(en_dict)\n",
        "cn_vocab_size = len(cn_dict)\n",
        "embed_size = hidden_size = 100\n",
        "dropout = 0.2\n",
        "\n",
        "encoder = Encoder(vocab_size=en_vocab_size, \n",
        "                  embed_size=embed_size, \n",
        "                  enc_hidden_size=hidden_size,\n",
        "                  dec_hidden_size=hidden_size,\n",
        "                  dropout=dropout)\n",
        "decoder = Decoder(vocab_size=cn_vocab_size, \n",
        "                  embed_size=embed_size, \n",
        "                  enc_hidden_size=hidden_size,\n",
        "                  dec_hidden_size=hidden_size,\n",
        "                  dropout=dropout)\n",
        "model = Seq2Seq(encoder, decoder)\n",
        "model = model.to(device)\n",
        "crit = LanguageModelCriterion().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIWrQvXIDmDH"
      },
      "source": [
        "def evaluate(model, data):\n",
        "    model.eval()\n",
        "    total_num_words = total_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        for it, (mb_x, mb_x_lengths, mb_y, mb_y_lengths) in enumerate(data):\n",
        "            mb_x = torch.from_numpy(mb_x).long().to(device)\n",
        "            mb_x_lengths = torch.from_numpy(mb_x_lengths).long().to(device)\n",
        "            mb_input = torch.from_numpy(mb_y[:,:-1]).long().to(device)\n",
        "            mb_out = torch.from_numpy(mb_y[:, 1:]).long().to(device)\n",
        "            mb_y_lengths = torch.from_numpy(mb_y_lengths-1).long().to(device)\n",
        "            mb_y_lengths[mb_y_lengths <= 0] = 1\n",
        "\n",
        "            mb_pred, attn = model(mb_x, mb_x_lengths, mb_input, mb_y_lengths)\n",
        "\n",
        "            mb_out_mask = torch.arange(mb_y_lengths.max().item(), device=device)[None, :] < mb_y_lengths[:, None]\n",
        "            mb_out_mask = mb_out_mask.float()\n",
        "            loss = crit(mb_pred, mb_out, mb_out_mask)\n",
        "\n",
        "            num_words = torch.sum(mb_y_lengths).item()\n",
        "            total_loss += loss.item() * num_words\n",
        "            total_num_words += num_words\n",
        "\n",
        "    print(\"evaluation loss\", total_loss/total_num_words)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ahrOkoWDrAg"
      },
      "source": [
        "def train(model, data, num_epochs=30):\n",
        "    for epoch in range(num_epochs):\n",
        "        total_num_words = total_loss = 0.\n",
        "        model.train()\n",
        "        for it, (mb_x, mb_x_lengths, mb_y, mb_y_lengths) in enumerate(data):\n",
        "            mb_x = torch.from_numpy(mb_x).long().to(device)\n",
        "            mb_x_lengths = torch.from_numpy(mb_x_lengths).long().to(device)\n",
        "            mb_input = torch.from_numpy(mb_y[:,:-1]).long().to(device)\n",
        "            mb_out = torch.from_numpy(mb_y[:, 1:]).long().to(device)\n",
        "            mb_y_lengths = torch.from_numpy(mb_y_lengths-1).long().to(device)\n",
        "            mb_y_lengths[mb_y_lengths <= 0] = 1\n",
        "\n",
        "            mb_pred, attn = model(mb_x, mb_x_lengths, mb_input, mb_y_lengths)\n",
        "\n",
        "            mb_out_mask = torch.arange(mb_y_lengths.max().item(), device=device)[None, :] < mb_y_lengths[:, None]\n",
        "            mb_out_mask = mb_out_mask.float()\n",
        "            loss = crit(mb_pred, mb_out, mb_out_mask)\n",
        "\n",
        "            num_words = torch.sum(mb_y_lengths).item()\n",
        "            total_loss += loss.item() * num_words\n",
        "            total_num_words += num_words\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.)\n",
        "            optimizer.step()\n",
        "\n",
        "            if it % 100 == 0:\n",
        "                print(\"epoch\", epoch, \"iteration\", it, \"loss\", loss.item())\n",
        "        print(\"epoch\", epoch, \"training loss\", total_loss/total_num_words)\n",
        "        if epoch % 5 == 0:\n",
        "            print(\"evaluating on dev...\")\n",
        "            evaluate(model, dev_data)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2niEVbFRDsrR",
        "outputId": "bf590017-492a-41e5-c2a6-6703a5ddfe63"
      },
      "source": [
        "train(model, train_data, num_epochs=10)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/Indexing.cu:937.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 iteration 0 loss 5.056572437286377\n",
            "epoch 0 iteration 100 loss 3.386885643005371\n",
            "epoch 0 iteration 200 loss 3.224148750305176\n",
            "epoch 0 iteration 300 loss 3.1893422603607178\n",
            "epoch 0 training loss 3.4214639735427017\n",
            "evaluating on dev...\n",
            "evaluation loss 3.348542815417564\n",
            "epoch 1 iteration 0 loss 3.2978861331939697\n",
            "epoch 1 iteration 100 loss 3.101950168609619\n",
            "epoch 1 iteration 200 loss 3.0656168460845947\n",
            "epoch 1 iteration 300 loss 3.073915719985962\n",
            "epoch 1 training loss 3.1280697253057625\n",
            "epoch 2 iteration 0 loss 3.191448211669922\n",
            "epoch 2 iteration 100 loss 2.9811811447143555\n",
            "epoch 2 iteration 200 loss 3.0011048316955566\n",
            "epoch 2 iteration 300 loss 3.024446725845337\n",
            "epoch 2 training loss 3.026451019602825\n",
            "epoch 3 iteration 0 loss 3.1155927181243896\n",
            "epoch 3 iteration 100 loss 2.8911142349243164\n",
            "epoch 3 iteration 200 loss 2.9621574878692627\n",
            "epoch 3 iteration 300 loss 3.0014398097991943\n",
            "epoch 3 training loss 2.9729169025646875\n",
            "epoch 4 iteration 0 loss 3.0676093101501465\n",
            "epoch 4 iteration 100 loss 2.8546037673950195\n",
            "epoch 4 iteration 200 loss 2.927562952041626\n",
            "epoch 4 iteration 300 loss 2.987788200378418\n",
            "epoch 4 training loss 2.9307734863033414\n",
            "epoch 5 iteration 0 loss 3.018080711364746\n",
            "epoch 5 iteration 100 loss 2.823894739151001\n",
            "epoch 5 iteration 200 loss 2.9075989723205566\n",
            "epoch 5 iteration 300 loss 2.973165988922119\n",
            "epoch 5 training loss 2.898478199321819\n",
            "evaluating on dev...\n",
            "evaluation loss 2.8364843644146838\n",
            "epoch 6 iteration 0 loss 2.989647626876831\n",
            "epoch 6 iteration 100 loss 2.8012032508850098\n",
            "epoch 6 iteration 200 loss 2.8917806148529053\n",
            "epoch 6 iteration 300 loss 2.962015390396118\n",
            "epoch 6 training loss 2.8779638853851495\n",
            "epoch 7 iteration 0 loss 2.9629828929901123\n",
            "epoch 7 iteration 100 loss 2.7844786643981934\n",
            "epoch 7 iteration 200 loss 2.878302574157715\n",
            "epoch 7 iteration 300 loss 2.9360344409942627\n",
            "epoch 7 training loss 2.854043012107207\n",
            "epoch 8 iteration 0 loss 2.9434149265289307\n",
            "epoch 8 iteration 100 loss 2.7828962802886963\n",
            "epoch 8 iteration 200 loss 2.862454652786255\n",
            "epoch 8 iteration 300 loss 2.918351888656616\n",
            "epoch 8 training loss 2.8343469262074064\n",
            "epoch 9 iteration 0 loss 2.922269582748413\n",
            "epoch 9 iteration 100 loss 2.7410316467285156\n",
            "epoch 9 iteration 200 loss 2.853746175765991\n",
            "epoch 9 iteration 300 loss 2.9103643894195557\n",
            "epoch 9 training loss 2.8152671586196307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rr1JPlFgHG8E"
      },
      "source": [
        "# model.summary()"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-e3r55b8DvrI",
        "outputId": "1d5bf299-c579-464d-fad7-0b03cc90b521"
      },
      "source": [
        "def translate_dev(i):\n",
        "    model.eval()\n",
        "    \n",
        "    en_sent = \" \".join([inv_en_dict[word] for word in dev_en[i]])\n",
        "    print(en_sent)\n",
        "    print(\" \".join([inv_cn_dict[word] for word in dev_cn[i]]))\n",
        "\n",
        "    sent = nltk.word_tokenize(en_sent.lower())\n",
        "    bos = torch.Tensor([[cn_dict[\"BOS\"]]]).long().to(device)\n",
        "    mb_x = torch.Tensor([[en_dict.get(w, 0) for w in sent]]).long().to(device)\n",
        "    mb_x_len = torch.Tensor([len(sent)]).long().to(device)\n",
        "    \n",
        "    translation, attention = model.translate(mb_x, mb_x_len, bos)\n",
        "    translation = [inv_cn_dict[i] for i in translation.data.cpu().numpy().reshape(-1)]\n",
        "\n",
        "    trans = []\n",
        "    for word in translation:\n",
        "        if word != \"EOS\":\n",
        "            trans.append(word)\n",
        "        else:\n",
        "            break\n",
        "    print(\" \".join(trans))\n",
        "for i in range(100,120):\n",
        "    translate_dev(i)\n",
        "    print()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOS tree street plant house skyscraper building van car wheel bus EOS\n",
            "BOS 公司 住宅区 临街院门 农林牧渔基地 中餐厅 汽车配件销售 生活服务场所 公交车站 物流速递 公共厕所 EOS\n",
            "中餐厅 生活服务场所 住宅区 停车场 临街院门 美容美发店 学校 物流速递 医药保健销售店 餐饮相关场所\n",
            "\n",
            "BOS tree boat building plant street window house car land palm EOS\n",
            "BOS 公司 中餐厅 生活服务场所 物流速递 家居建材市场 农林牧渔基地 住宅区 宾馆酒店 专卖店 娱乐场所 EOS\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/Indexing.cu:937.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "停车场 临街院门 生活服务场所 公司 农林牧渔基地 公交车站 住宅区 交通地名 交通地名 公交车站\n",
            "\n",
            "BOS tree street car plant wheel train house window land building EOS\n",
            "BOS 公司 中餐厅 物流速递 农林牧渔基地 家居建材市场 生活服务场所 临街院门 住宅区 宾馆酒店 大众特约销售 EOS\n",
            "公司 临街院门 农林牧渔基地 生活服务场所 中餐厅 政府机关 美容美发店 公共厕所 物流速递 公交车站\n",
            "\n",
            "BOS tree wheel building plant car truck street train land boat EOS\n",
            "BOS 公司 农林牧渔基地 临街院门 住宅区 汽车配件销售 生活服务场所 诊所 政府机关 美容美发店 中餐厅 EOS\n",
            "中餐厅 生活服务场所 临街院门 住宅区 美容美发店 学校 餐饮相关场所 医药保健销售店 娱乐场所 彩票彩券销售点\n",
            "\n",
            "BOS tree window wheel car building truck street boat land bus EOS\n",
            "BOS 公司 临街院门 农林牧渔基地 产业园区 生活服务场所 政府机关 住宅区 加油站 公共厕所 汽车配件销售 EOS\n",
            "生活服务场所 中餐厅 临街院门 住宅区 美容美发店 学校 餐饮相关场所 医药保健销售店 物流速递 医药保健销售店\n",
            "\n",
            "BOS tree window car house wheel building vehicle street skyscraper plant EOS\n",
            "BOS 农林牧渔基地 临街院门 住宅区 公司 生活服务场所 停车场 公交车站 商务住宅相关 物流速递 产业园区 EOS\n",
            "生活服务场所 中餐厅 临街院门 住宅区 停车场 美容美发店 中介机构 医药保健销售店 彩票彩券销售点 彩票彩券销售点\n",
            "\n",
            "BOS tree street train plant wheel boat building car house person EOS\n",
            "BOS 公司 中餐厅 物流速递 生活服务场所 家居建材市场 农林牧渔基地 住宅区 宾馆酒店 临街院门 彩票彩券销售点 EOS\n",
            "公司 临街院门 农林牧渔基地 生活服务场所 交通地名 中餐厅 政府机关 工厂 交通地名 公交车站\n",
            "\n",
            "BOS window tree building skyscraper street plant car wheel land traffic EOS\n",
            "BOS 洗浴推拿场所 中餐厅 美容美发店 宾馆酒店 生活服务场所 医药保健销售店 快餐厅 物流速递 诊所 学校 EOS\n",
            "中餐厅 生活服务场所 美容美发店 临街院门 住宅区 停车场 娱乐场所 医药保健销售店 彩票彩券销售点 彩票彩券销售点\n",
            "\n",
            "BOS tree street window building house plant land car boat footwear EOS\n",
            "BOS 中餐厅 生活服务场所 政府机关 住宅区 临街院门 公交车站 物流速递 美容美发店 农林牧渔基地 停车场 EOS\n",
            "中餐厅 生活服务场所 临街院门 住宅区 美容美发店 学校 餐饮相关场所 医药保健销售店 维修站点 学校\n",
            "\n",
            "BOS tree car street building plant land wheel vehicle boat clothing EOS\n",
            "BOS 物流速递 生活服务场所 停车场 中餐厅 农林牧渔基地 临街院门 公司 住宅区 大众特约销售 交通地名 EOS\n",
            "生活服务场所 中餐厅 住宅区 临街院门 停车场 美容美发店 学校 自动提款机 物流速递 医药保健销售店\n",
            "\n",
            "BOS tree street boat car wheel vehicle train skyscraper window plant EOS\n",
            "BOS 学校 生活服务场所 住宅区 临街院门 停车场 驾校 交通地名 科教文化场所 建筑物门 政府机关 EOS\n",
            "公司 临街院门 农林牧渔基地 交通地名 生活服务场所 交通地名 中餐厅 公交车站 交通地名 住宅区\n",
            "\n",
            "BOS tree street train window building boat skyscraper wheel vehicle car EOS\n",
            "BOS 学校 生活服务场所 临街院门 住宅区 停车场 驾校 交通地名 科教文化场所 建筑物门 政府机关 EOS\n",
            "停车场 临街院门 生活服务场所 住宅区 中餐厅 公交车站 公司 交通地名 农林牧渔基地 交通地名\n",
            "\n",
            "BOS tree street skyscraper building car wheel boat train window vehicle EOS\n",
            "BOS 公司 临街院门 汽车配件销售 驾校 生活服务场所 科研机构 交通地名 培训机构 停车场 农林牧渔基地 EOS\n",
            "停车场 临街院门 住宅区 生活服务场所 公交车站 公司 交通地名 农林牧渔基地 交通地名 公交车站\n",
            "\n",
            "BOS tree street boat building car skyscraper wheel land vehicle window EOS\n",
            "BOS 公司 临街院门 汽车配件销售 生活服务场所 科研机构 交通地名 驾校 培训机构 科教文化场所 停车场 EOS\n",
            "停车场 临街院门 生活服务场所 住宅区 公交车站 公司 农林牧渔基地 交通地名 公交车站 交通地名\n",
            "\n",
            "BOS street tree building car wheel skyscraper boat vehicle train land EOS\n",
            "BOS 公司 汽车配件销售 临街院门 科研机构 培训机构 停车场 交通地名 住宅区 福特特约维修 科教文化场所 EOS\n",
            "停车场 临街院门 生活服务场所 住宅区 中餐厅 公交车站 公司 交通地名 农林牧渔基地 交通地名\n",
            "\n",
            "BOS tree skyscraper window plant building train car wheel street vehicle EOS\n",
            "BOS 生活服务场所 公司 临街院门 中餐厅 农林牧渔基地 美容美发店 停车场 培训机构 科研机构 公检法机构 EOS\n",
            "停车场 临街院门 住宅区 生活服务场所 中餐厅 公交车站 公司 公交车站 农林牧渔基地 交通地名\n",
            "\n",
            "BOS tree skyscraper window street car wheel building vehicle train land EOS\n",
            "BOS 中餐厅 生活服务场所 美容美发店 科教文化场所 学校 临街院门 诊所 停车场 农林牧渔基地 洗浴推拿场所 EOS\n",
            "停车场 临街院门 住宅区 生活服务场所 中餐厅 公交车站 公司 公交车站 交通地名 农林牧渔基地\n",
            "\n",
            "BOS window car skyscraper wheel building plant street tree vehicle bus EOS\n",
            "BOS 中餐厅 公司 美容美发店 生活服务场所 银行 娱乐场所 临街院门 自动提款机 宾馆酒店 停车场 EOS\n",
            "中餐厅 生活服务场所 美容美发店 临街院门 住宅区 停车场 医药保健销售店 餐饮相关场所 中介机构 彩票彩券销售点\n",
            "\n",
            "BOS window car wheel tree skyscraper plant building vehicle street train EOS\n",
            "BOS 公司 中餐厅 生活服务场所 临街院门 银行 娱乐场所 美容美发店 停车场 宾馆酒店 自动提款机 EOS\n",
            "生活服务场所 中餐厅 美容美发店 临街院门 住宅区 停车场 医药保健销售店 餐饮相关场所 中介机构 彩票彩券销售点\n",
            "\n",
            "BOS tree car wheel window street building vehicle skyscraper train boat EOS\n",
            "BOS 科教文化场所 农林牧渔基地 临街院门 生活服务场所 学校 中餐厅 停车场 产业园区 茶艺馆 住宅区 EOS\n",
            "生活服务场所 中餐厅 美容美发店 临街院门 住宅区 停车场 餐饮相关场所 医药保健销售店 学校 娱乐场所\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lBs7bvIHfUC"
      },
      "source": [
        "torch.save(model,'/content/drive/MyDrive/seq2seq.pt')"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZ7l4LAGHslv",
        "outputId": "d1956616-9852-476a-9d50-1fb3e22b9d81"
      },
      "source": [
        "!pip install rouge-score\n",
        "from rouge_score import rouge_scorer\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import re\n",
        "\n",
        "from nltk.stem import porter\n",
        "import six\n",
        "from six.moves import map\n",
        "from six.moves import range\n",
        "from rouge_score import scoring\n",
        "# from rouge_score import tokenize\n",
        "\n",
        "\n",
        "class RougeScorer(scoring.BaseScorer):\n",
        "  \"\"\"Calculate rouges scores between two blobs of text.\n",
        "\n",
        "  Sample usage:\n",
        "    scorer = RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "    scores = scorer.score('The quick brown fox jumps over the lazy dog',\n",
        "                          'The quick brown dog jumps on the log.')\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, rouge_types, use_stemmer=False):\n",
        "    \"\"\"Initializes a new RougeScorer.\n",
        "\n",
        "    Valid rouge types that can be computed are:\n",
        "      rougen (e.g. rouge1, rouge2): n-gram based scoring.\n",
        "      rougeL: Longest common subsequence based scoring.\n",
        "\n",
        "    Args:\n",
        "      rouge_types: A list of rouge types to calculate.\n",
        "      use_stemmer: Bool indicating whether Porter stemmer should be used to\n",
        "        strip word suffixes to improve matching.\n",
        "    Returns:\n",
        "      A dict mapping rouge types to Score tuples.\n",
        "    \"\"\"\n",
        "\n",
        "    self.rouge_types = rouge_types\n",
        "    self._stemmer = porter.PorterStemmer() if use_stemmer else None\n",
        "\n",
        "  def score(self, target, prediction):\n",
        "    \"\"\"Calculates rouge scores between the target and prediction.\n",
        "\n",
        "    Args:\n",
        "      target: Text containing the target (ground truth) text.\n",
        "      prediction: Text containing the predicted text.\n",
        "    Returns:\n",
        "      A dict mapping each rouge type to a Score object.\n",
        "    Raises:\n",
        "      ValueError: If an invalid rouge type is encountered.\n",
        "    \"\"\"\n",
        "\n",
        "    target_tokens = tokenize(target, self._stemmer)\n",
        "    prediction_tokens = tokenize(prediction, self._stemmer)\n",
        "    result = {}\n",
        "\n",
        "    for rouge_type in self.rouge_types:\n",
        "      if rouge_type == \"rougeL\":\n",
        "        # Rouge from longest common subsequences.\n",
        "        scores = _score_lcs(target_tokens, prediction_tokens)\n",
        "      elif rouge_type == \"rougeLsum\":\n",
        "        # Note: Does not support multi-line text.\n",
        "        def get_sents(text):\n",
        "          # Assume sentences are separated by newline.\n",
        "          sents = six.ensure_str(text).split(\"\\n\")\n",
        "          sents = [x for x in sents if len(x)]\n",
        "          return sents\n",
        "\n",
        "        target_tokens_list = [\n",
        "            tokenize(s, self._stemmer) for s in get_sents(target)]\n",
        "        prediction_tokens_list = [\n",
        "            tokenize(s, self._stemmer) for s in get_sents(prediction)]\n",
        "        scores = _summary_level_lcs(target_tokens_list,\n",
        "                                    prediction_tokens_list)\n",
        "      elif re.match(r\"rouge[0-9]$\", six.ensure_str(rouge_type)):\n",
        "        # Rouge from n-grams.\n",
        "        n = int(rouge_type[5:])\n",
        "        if n <= 0:\n",
        "          raise ValueError(\"rougen requires positive n: %s\" % rouge_type)\n",
        "        target_ngrams = _create_ngrams(target_tokens, n)\n",
        "        prediction_ngrams = _create_ngrams(prediction_tokens, n)\n",
        "        scores = _score_ngrams(target_ngrams, prediction_ngrams)\n",
        "      else:\n",
        "        raise ValueError(\"Invalid rouge type: %s\" % rouge_type)\n",
        "      result[rouge_type] = scores\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def _create_ngrams(tokens, n):\n",
        "  \"\"\"Creates ngrams from the given list of tokens.\n",
        "\n",
        "  Args:\n",
        "    tokens: A list of tokens from which ngrams are created.\n",
        "    n: Number of tokens to use, e.g. 2 for bigrams.\n",
        "  Returns:\n",
        "    A dictionary mapping each bigram to the number of occurrences.\n",
        "  \"\"\"\n",
        "\n",
        "  ngrams = collections.Counter()\n",
        "  for ngram in (tuple(tokens[i:i + n]) for i in range(len(tokens) - n + 1)):\n",
        "    ngrams[ngram] += 1\n",
        "  return ngrams\n",
        "\n",
        "\n",
        "def _score_lcs(target_tokens, prediction_tokens):\n",
        "  \"\"\"Computes LCS (Longest Common Subsequence) rouge scores.\n",
        "\n",
        "  Args:\n",
        "    target_tokens: Tokens from the target text.\n",
        "    prediction_tokens: Tokens from the predicted text.\n",
        "  Returns:\n",
        "    A Score object containing computed scores.\n",
        "  \"\"\"\n",
        "\n",
        "  if not target_tokens or not prediction_tokens:\n",
        "    return scoring.Score(precision=0, recall=0, fmeasure=0)\n",
        "\n",
        "  # Compute length of LCS from the bottom up in a table (DP appproach).\n",
        "  lcs_table = _lcs_table(target_tokens, prediction_tokens)\n",
        "  lcs_length = lcs_table[-1][-1]\n",
        "\n",
        "  precision = lcs_length / len(prediction_tokens)\n",
        "  recall = lcs_length / len(target_tokens)\n",
        "  fmeasure = scoring.fmeasure(precision, recall)\n",
        "\n",
        "  return scoring.Score(precision=precision, recall=recall, fmeasure=fmeasure)\n",
        "\n",
        "\n",
        "def _lcs_table(ref, can):\n",
        "  \"\"\"Create 2-d LCS score table.\"\"\"\n",
        "  rows = len(ref)\n",
        "  cols = len(can)\n",
        "  lcs_table = [[0] * (cols + 1) for _ in range(rows + 1)]\n",
        "  for i in range(1, rows + 1):\n",
        "    for j in range(1, cols + 1):\n",
        "      if ref[i - 1] == can[j - 1]:\n",
        "        lcs_table[i][j] = lcs_table[i - 1][j - 1] + 1\n",
        "      else:\n",
        "        lcs_table[i][j] = max(lcs_table[i - 1][j], lcs_table[i][j - 1])\n",
        "  return lcs_table\n",
        "\n",
        "\n",
        "def _backtrack_norec(t, ref, can):\n",
        "  \"\"\"Read out LCS.\"\"\"\n",
        "  i = len(ref)\n",
        "  j = len(can)\n",
        "  lcs = []\n",
        "  while i > 0 and j > 0:\n",
        "    if ref[i - 1] == can[j - 1]:\n",
        "      lcs.insert(0, i-1)\n",
        "      i -= 1\n",
        "      j -= 1\n",
        "    elif t[i][j - 1] > t[i - 1][j]:\n",
        "      j -= 1\n",
        "    else:\n",
        "      i -= 1\n",
        "  return lcs\n",
        "\n",
        "\n",
        "def _summary_level_lcs(ref_sent, can_sent):\n",
        "  \"\"\"ROUGE: Summary-level LCS, section 3.2 in ROUGE paper.\n",
        "\n",
        "  Args:\n",
        "    ref_sent: list of tokenized reference sentences\n",
        "    can_sent: list of tokenized candidate sentences\n",
        "\n",
        "  Returns:\n",
        "    summary level ROUGE score\n",
        "  \"\"\"\n",
        "  if not ref_sent or not can_sent:\n",
        "    return scoring.Score(precision=0, recall=0, fmeasure=0)\n",
        "\n",
        "  m = sum(map(len, ref_sent))\n",
        "  n = sum(map(len, can_sent))\n",
        "  if not n or not m:\n",
        "    return scoring.Score(precision=0, recall=0, fmeasure=0)\n",
        "\n",
        "  # get token counts to prevent double counting\n",
        "  token_cnts_r = collections.Counter()\n",
        "  token_cnts_c = collections.Counter()\n",
        "  for s in ref_sent:\n",
        "    # s is a list of tokens\n",
        "    token_cnts_r.update(s)\n",
        "  for s in can_sent:\n",
        "    token_cnts_c.update(s)\n",
        "\n",
        "  hits = 0\n",
        "  for r in ref_sent:\n",
        "    lcs = _union_lcs(r, can_sent)\n",
        "    # Prevent double-counting:\n",
        "    # The paper describes just computing hits += len(_union_lcs()),\n",
        "    # but the implementation prevents double counting. We also\n",
        "    # implement this as in version 1.5.5.\n",
        "    for t in lcs:\n",
        "      if token_cnts_c[t] > 0 and token_cnts_r[t] > 0:\n",
        "        hits += 1\n",
        "        token_cnts_c[t] -= 1\n",
        "        token_cnts_r[t] -= 1\n",
        "\n",
        "  recall = hits / m\n",
        "  precision = hits / n\n",
        "  fmeasure = scoring.fmeasure(precision, recall)\n",
        "  return scoring.Score(precision=precision, recall=recall, fmeasure=fmeasure)\n",
        "\n",
        "\n",
        "def _union_lcs(ref, c_list):\n",
        "  \"\"\"Find union LCS between a ref sentence and list of candidate sentences.\n",
        "\n",
        "  Args:\n",
        "    ref: list of tokens\n",
        "    c_list: list of list of indices for LCS into reference summary\n",
        "\n",
        "  Returns:\n",
        "    List of tokens in ref representing union LCS.\n",
        "  \"\"\"\n",
        "  lcs_list = [lcs_ind(ref, c) for c in c_list]\n",
        "  return [ref[i] for i in _find_union(lcs_list)]\n",
        "\n",
        "\n",
        "def _find_union(lcs_list):\n",
        "  \"\"\"Finds union LCS given a list of LCS.\"\"\"\n",
        "  return sorted(list(set().union(*lcs_list)))\n",
        "\n",
        "\n",
        "def lcs_ind(ref, can):\n",
        "  \"\"\"Returns one of the longest lcs.\"\"\"\n",
        "  t = _lcs_table(ref, can)\n",
        "  return _backtrack_norec(t, ref, can)\n",
        "\n",
        "\n",
        "def _score_ngrams(target_ngrams, prediction_ngrams):\n",
        "  \"\"\"Compute n-gram based rouge scores.\n",
        "\n",
        "  Args:\n",
        "    target_ngrams: A Counter object mapping each ngram to number of\n",
        "      occurrences for the target text.\n",
        "    prediction_ngrams: A Counter object mapping each ngram to number of\n",
        "      occurrences for the prediction text.\n",
        "  Returns:\n",
        "    A Score object containing computed scores.\n",
        "  \"\"\"\n",
        "\n",
        "  intersection_ngrams_count = 0\n",
        "  for ngram in six.iterkeys(target_ngrams):\n",
        "    intersection_ngrams_count += min(target_ngrams[ngram],\n",
        "                                     prediction_ngrams[ngram])\n",
        "  target_ngrams_count = sum(target_ngrams.values())\n",
        "  prediction_ngrams_count = sum(prediction_ngrams.values())\n",
        "\n",
        "  precision = intersection_ngrams_count / max(prediction_ngrams_count, 1)\n",
        "  recall = intersection_ngrams_count / max(target_ngrams_count, 1)\n",
        "  fmeasure = scoring.fmeasure(precision, recall)\n",
        "\n",
        "  return scoring.Score(precision=precision, recall=recall, fmeasure=fmeasure)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge-score) (3.2.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.19.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (0.12.0)\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6VIJxVGH7Sl"
      },
      "source": [
        "def tokenize(text, stemmer):\n",
        "  \"\"\"Tokenize input text into a list of tokens.\n",
        "\n",
        "  This approach aims to replicate the approach taken by Chin-Yew Lin in\n",
        "  the original ROUGE implementation.\n",
        "\n",
        "  Args:\n",
        "    text: A text blob to tokenize.\n",
        "    stemmer: An optional stemmer.\n",
        "\n",
        "  Returns:\n",
        "    A list of string tokens extracted from input text.\n",
        "  \"\"\"\n",
        "\n",
        "  # Convert everything to lowercase.\n",
        "  text = text.lower()\n",
        "  # Replace any non-alpha-numeric characters with spaces.\n",
        "  # text = re.sub(r\"[^a-z0-9]+\", \" \", six.ensure_str(text))\n",
        "  # ^ [ / u4E00 - / u9FFF]+$\n",
        "  tokens = re.split(r\"\\s+\", text)\n",
        "\n",
        "  # print(tokens)\n",
        "  # print(1)\n",
        "  if stemmer:\n",
        "    # Only stem words more than 3 characters long.\n",
        "    tokens = [stemmer.stem(x) if len(x) > 3 else x for x in tokens]\n",
        "\n",
        "  # print(tokens)\n",
        "  # print(2)\n",
        "  # print(six.ensure_str(x))\n",
        "  # One final check to drop any empty or invalid tokens.\n",
        "  # tokens = [x for x in tokens if re.match(r\"^[/u4E00 - /u9FFF]+$\", six.ensure_str(x))]\n",
        "  # tokens = [x for x in tokens if re.match(r\"^[a-z0-9]+$\", six.ensure_str(x))]\n",
        "  tokens=[x for x in tokens if six.ensure_str]\n",
        "\n",
        "\n",
        "  # print(tokens)\n",
        "  # print(3)\n",
        "  return tokens"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RByD5156IWBJ"
      },
      "source": [
        "### 掩盖验证函数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT4AfEQRIVuK"
      },
      "source": [
        "def translate_dev(i):\n",
        "    model.eval()\n",
        "    \n",
        "    en_sent = \" \".join([inv_en_dict[word] for word in dev_en[i]])\n",
        "    # print(en_sent)\n",
        "    \n",
        "    # print(\" \".join([inv_cn_dict[word] for word in dev_cn[i]]))\n",
        "\n",
        "    sent = nltk.word_tokenize(en_sent.lower())\n",
        "    bos = torch.Tensor([[cn_dict[\"BOS\"]]]).long().to(device)\n",
        "    mb_x = torch.Tensor([[en_dict.get(w, 0) for w in sent]]).long().to(device)\n",
        "    mb_x_len = torch.Tensor([len(sent)]).long().to(device)\n",
        "    \n",
        "    translation, attention = model.translate(mb_x, mb_x_len, bos)\n",
        "    translation = [inv_cn_dict[i] for i in translation.data.cpu().numpy().reshape(-1)]\n",
        "\n",
        "    trans = []\n",
        "    for word in translation:\n",
        "        if word != \"EOS\":\n",
        "            trans.append(word)\n",
        "        else:\n",
        "            break\n",
        "    # print(\" \".join(trans))\n",
        "    return en_sent,\" \".join([inv_cn_dict[word] for word in dev_cn[i]]),\" \".join(trans)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T4mq5gnIeC7"
      },
      "source": [
        "# translate_dev(1)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XveZedTyH9wm",
        "outputId": "b33e246b-38fd-4b62-826f-3255e7adbbbb"
      },
      "source": [
        "A=[]\n",
        "B=[]\n",
        "C=[]\n",
        "scorer = RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "for i in range(300):\n",
        "  a,b,c=translate_dev(i)\n",
        "  scores =  scorer.score(b,c)\n",
        "  # print(scores['rouge1'])\n",
        "  tmp=scores['rouge1']\n",
        "  A.append(tmp[0])\n",
        "  B.append(tmp[1])\n",
        "  C.append(tmp[2])"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/Indexing.cu:937.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEv-5HyZOZUI",
        "outputId": "e8a1ce5b-19a3-45fb-f981-be03216b6fc1"
      },
      "source": [
        "import numpy as np\n",
        "print(np.mean(A))\n",
        "print(np.mean(B))\n",
        "print(np.mean(C))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4726666666666666\n",
            "0.3938888888888889\n",
            "0.4296969696969697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pk0RIeKObE9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}