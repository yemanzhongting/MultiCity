{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM做翻译.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1rfGRi7RRhaobPtigjmob_WVqz-K8Wpts",
      "authorship_tag": "ABX9TyNPpFkiRSvTSEmYJ4+NxJTi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yemanzhongting/MultiCity/blob/main/LSTM%E5%81%9A%E7%BF%BB%E8%AF%91.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13POGVO9xyEi",
        "outputId": "16a30eb6-cb9b-49db-a04c-71ab1ef27b52"
      },
      "source": [
        "data_path = '/content/drive/MyDrive/data/sub-sv.txt'\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "data_write_path = '/content/drive/MyDrive/data/sv-sub.txt'\n",
        "with open(data_write_path, 'w', encoding='utf-8') as f:\n",
        "    for i in lines:\n",
        "      tmp=i.split('\\t')\n",
        "      try:\n",
        "        f.write(tmp[1]+'\\t'+tmp[0]+'\\n')\n",
        "      except:\n",
        "        print(i)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fP9GDfw6iu-"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "batch_size = 64  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 10000  # Number of samples to train on.\n",
        "# Path to the data txt file on disk.\n",
        "data_path = '/content/drive/MyDrive/data/sv-sub.txt'\n",
        "\n",
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "for line in lines[: min(len(lines), len(lines) - 1)]:\n",
        "    input_text, target_text = line.split('\\t')\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    #我们修改一下顺序，感知增强\n",
        "\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    # input_texts.append(target_text)\n",
        "    # target_texts.append(input_text)\n",
        "\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1NTJiWeySTj",
        "outputId": "fe96de0a-f4d2-4134-c6c7-d2fd0199157d"
      },
      "source": [
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print('Number of samples:', len(input_texts))\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 20790\n",
            "Number of unique input tokens: 43\n",
            "Number of unique output tokens: 286\n",
            "Max sequence length for inputs: 80\n",
            "Max sequence length for outputs: 71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsCasY_Lx46v"
      },
      "source": [
        "# mapping token to index， easily to vectors\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "# np.zeros(shape, dtype, order)\n",
        "# shape is an tuple, in here 3D\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "\n",
        "# input_texts contain all english sentences\n",
        "# output_texts contain all chinese sentences\n",
        "# zip('ABC','xyz') ==> Ax By Cz, looks like that\n",
        "# the aim is: vectorilize text, 3D\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        # 3D vector only z-index has char its value equals 1.0\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            # igone t=0 and start t=1, means \n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbEpJQFVx7SM"
      },
      "source": [
        "# Define an input sequence and process it.\n",
        "# input prodocts keras tensor, to fit keras model!\n",
        "# 1x73 vector \n",
        "# encoder_inputs is a 1x73 tensor!\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "\n",
        "# units=256, return the last state in addition to the output\n",
        "encoder_lstm = LSTM((latent_dim), return_state=True)\n",
        "\n",
        "# LSTM(tensor) return output, state-history, state-current\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
        "\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KN9i5V4Px7qZ",
        "outputId": "ed6b3f60-bc13-4962-d217-f47db9732ba1"
      },
      "source": [
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM((latent_dim), return_sequences=True, return_state=True)\n",
        "\n",
        "# obtain output\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,initial_state=encoder_states)\n",
        "\n",
        "# dense 2580x1 units full connented layer\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "\n",
        "# why let decoder_outputs go through dense ?\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn, groups layers into an object \n",
        "# with training and inference features\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "# model(input, output)\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Run training\n",
        "# compile -> configure model for training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "# model optimizsm\n",
        "model.fit([encoder_input_data, decoder_input_data], \n",
        "          decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=20,\n",
        "          validation_split=0.2)\n",
        "# Save model\n",
        "model.save('seq2seq.h5')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "260/260 [==============================] - 22s 70ms/step - loss: 2.5734 - val_loss: 1.9825\n",
            "Epoch 2/20\n",
            "260/260 [==============================] - 17s 65ms/step - loss: 1.4941 - val_loss: 1.4821\n",
            "Epoch 3/20\n",
            "260/260 [==============================] - 17s 66ms/step - loss: 1.1317 - val_loss: 0.9252\n",
            "Epoch 4/20\n",
            "260/260 [==============================] - 17s 65ms/step - loss: 0.9591 - val_loss: 0.9399\n",
            "Epoch 5/20\n",
            "260/260 [==============================] - 17s 65ms/step - loss: 0.8649 - val_loss: 0.9852\n",
            "Epoch 6/20\n",
            "260/260 [==============================] - 17s 65ms/step - loss: 0.8014 - val_loss: 0.9537\n",
            "Epoch 7/20\n",
            "260/260 [==============================] - 17s 65ms/step - loss: 0.7514 - val_loss: 0.9445\n",
            "Epoch 8/20\n",
            "260/260 [==============================] - 17s 65ms/step - loss: 0.7462 - val_loss: 0.6563\n",
            "Epoch 9/20\n",
            "260/260 [==============================] - 17s 66ms/step - loss: 0.7296 - val_loss: 0.6470\n",
            "Epoch 10/20\n",
            "260/260 [==============================] - 17s 65ms/step - loss: 0.7256 - val_loss: 0.6574\n",
            "Epoch 11/20\n",
            "260/260 [==============================] - 17s 65ms/step - loss: 0.6999 - val_loss: 0.8748\n",
            "Epoch 12/20\n",
            "260/260 [==============================] - 17s 65ms/step - loss: 0.6598 - val_loss: 0.6235\n",
            "Epoch 13/20\n",
            "260/260 [==============================] - 17s 65ms/step - loss: 0.6742 - val_loss: 0.6094\n",
            "Epoch 14/20\n",
            "260/260 [==============================] - 17s 66ms/step - loss: 0.6562 - val_loss: 0.6053\n",
            "Epoch 15/20\n",
            "260/260 [==============================] - 17s 65ms/step - loss: 0.6444 - val_loss: 0.6572\n",
            "Epoch 16/20\n",
            "260/260 [==============================] - 17s 66ms/step - loss: 0.6505 - val_loss: 0.8597\n",
            "Epoch 17/20\n",
            "260/260 [==============================] - 17s 65ms/step - loss: 0.6401 - val_loss: 0.5986\n",
            "Epoch 18/20\n",
            "260/260 [==============================] - 17s 65ms/step - loss: 0.6140 - val_loss: 0.5859\n",
            "Epoch 19/20\n",
            "260/260 [==============================] - 17s 65ms/step - loss: 0.6356 - val_loss: 0.5850\n",
            "Epoch 20/20\n",
            "260/260 [==============================] - 17s 66ms/step - loss: 0.6269 - val_loss: 0.5889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lm5IiaKH8JGJ",
        "outputId": "808c3b56-2a44-4617-b9d8-e981d83cf07e"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Sep 10 09:10:32 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   72C    P0    90W / 149W |   8683MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFFT4BJvx9F2"
      },
      "source": [
        "# Define sampling models\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict(\n",
        "    (i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict(\n",
        "    (i, char) for char, i in target_token_index.items())\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "    # this target_seq you can treat as initial state\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        # argmax: Returns the indices of the maximum values along an axis\n",
        "        # just like find the most possible char\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        # find char using index\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        # and append sentence\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        # append then ?\n",
        "        # creating another new target_seq\n",
        "        # and this time assume sampled_token_index to 1.0\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        # Update states\n",
        "        # update states, frome the front parts\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UP57Cda-0qwa",
        "outputId": "958562d0-80e5-4bb5-e41d-b81b9bf2c65f"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None, 43)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None, 286)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 256), (None, 307200      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 256),  556032      input_2[0][0]                    \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 286)    73502       lstm_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 936,734\n",
            "Trainable params: 936,734\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQ1gUpE2x_EL",
        "outputId": "111d07e4-3c86-4124-a5d3-a86971292692"
      },
      "source": [
        "for seq_index in range(0,1):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "\n",
        "    print(target_texts[seq_index])\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('Input sentence:', input_texts[seq_index])\n",
        "    print('Decoded sentence:', decoded_sentence)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t物流速递 住宅区 美容美发店 公司 中餐厅 诊所 政府机关 福特特约维修 临街院门 交通地名\n",
            "\n",
            "Input sentence: Tree Plant House Window Car Land Street Billboard Tire\n",
            "Decoded sentence: 中餐厅 生活服务场所 美容美发店 临街院门 住宅区 医药保健销售店 餐饮相关场所 医药保健销售店 洗浴推拿场所 公司\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsKPEArC_BZ5"
      },
      "source": [
        "A=[]\n",
        "B=[]\n",
        "C=[]\n",
        "scorer = RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "for seq_index in range(0,300):\n",
        "  input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "\n",
        "  # print(target_texts[seq_index])\n",
        "  decoded_sentence = decode_sequence(input_seq)\n",
        "  scores =  scorer.score(target_texts[seq_index],decoded_sentence)\n",
        "  # print(scores['rouge1'])\n",
        "  tmp=scores['rouge1']\n",
        "  A.append(tmp[0])\n",
        "  B.append(tmp[1])\n",
        "  C.append(tmp[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZwA0VnVBIF-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LjrHveUBIIz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wk908UVK-Y5N",
        "outputId": "4412f6f8-3117-439d-afd3-75d8b527ad0c"
      },
      "source": [
        "!pip install rouge-score\n",
        "from rouge_score import rouge_scorer\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import re\n",
        "\n",
        "from nltk.stem import porter\n",
        "import six\n",
        "from six.moves import map\n",
        "from six.moves import range\n",
        "from rouge_score import scoring\n",
        "# from rouge_score import tokenize\n",
        "\n",
        "\n",
        "class RougeScorer(scoring.BaseScorer):\n",
        "  \"\"\"Calculate rouges scores between two blobs of text.\n",
        "\n",
        "  Sample usage:\n",
        "    scorer = RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "    scores = scorer.score('The quick brown fox jumps over the lazy dog',\n",
        "                          'The quick brown dog jumps on the log.')\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, rouge_types, use_stemmer=False):\n",
        "    \"\"\"Initializes a new RougeScorer.\n",
        "\n",
        "    Valid rouge types that can be computed are:\n",
        "      rougen (e.g. rouge1, rouge2): n-gram based scoring.\n",
        "      rougeL: Longest common subsequence based scoring.\n",
        "\n",
        "    Args:\n",
        "      rouge_types: A list of rouge types to calculate.\n",
        "      use_stemmer: Bool indicating whether Porter stemmer should be used to\n",
        "        strip word suffixes to improve matching.\n",
        "    Returns:\n",
        "      A dict mapping rouge types to Score tuples.\n",
        "    \"\"\"\n",
        "\n",
        "    self.rouge_types = rouge_types\n",
        "    self._stemmer = porter.PorterStemmer() if use_stemmer else None\n",
        "\n",
        "  def score(self, target, prediction):\n",
        "    \"\"\"Calculates rouge scores between the target and prediction.\n",
        "\n",
        "    Args:\n",
        "      target: Text containing the target (ground truth) text.\n",
        "      prediction: Text containing the predicted text.\n",
        "    Returns:\n",
        "      A dict mapping each rouge type to a Score object.\n",
        "    Raises:\n",
        "      ValueError: If an invalid rouge type is encountered.\n",
        "    \"\"\"\n",
        "\n",
        "    target_tokens = tokenize(target, self._stemmer)\n",
        "    prediction_tokens = tokenize(prediction, self._stemmer)\n",
        "    result = {}\n",
        "\n",
        "    for rouge_type in self.rouge_types:\n",
        "      if rouge_type == \"rougeL\":\n",
        "        # Rouge from longest common subsequences.\n",
        "        scores = _score_lcs(target_tokens, prediction_tokens)\n",
        "      elif rouge_type == \"rougeLsum\":\n",
        "        # Note: Does not support multi-line text.\n",
        "        def get_sents(text):\n",
        "          # Assume sentences are separated by newline.\n",
        "          sents = six.ensure_str(text).split(\"\\n\")\n",
        "          sents = [x for x in sents if len(x)]\n",
        "          return sents\n",
        "\n",
        "        target_tokens_list = [\n",
        "            tokenize(s, self._stemmer) for s in get_sents(target)]\n",
        "        prediction_tokens_list = [\n",
        "            tokenize(s, self._stemmer) for s in get_sents(prediction)]\n",
        "        scores = _summary_level_lcs(target_tokens_list,\n",
        "                                    prediction_tokens_list)\n",
        "      elif re.match(r\"rouge[0-9]$\", six.ensure_str(rouge_type)):\n",
        "        # Rouge from n-grams.\n",
        "        n = int(rouge_type[5:])\n",
        "        if n <= 0:\n",
        "          raise ValueError(\"rougen requires positive n: %s\" % rouge_type)\n",
        "        target_ngrams = _create_ngrams(target_tokens, n)\n",
        "        prediction_ngrams = _create_ngrams(prediction_tokens, n)\n",
        "        scores = _score_ngrams(target_ngrams, prediction_ngrams)\n",
        "      else:\n",
        "        raise ValueError(\"Invalid rouge type: %s\" % rouge_type)\n",
        "      result[rouge_type] = scores\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def _create_ngrams(tokens, n):\n",
        "  \"\"\"Creates ngrams from the given list of tokens.\n",
        "\n",
        "  Args:\n",
        "    tokens: A list of tokens from which ngrams are created.\n",
        "    n: Number of tokens to use, e.g. 2 for bigrams.\n",
        "  Returns:\n",
        "    A dictionary mapping each bigram to the number of occurrences.\n",
        "  \"\"\"\n",
        "\n",
        "  ngrams = collections.Counter()\n",
        "  for ngram in (tuple(tokens[i:i + n]) for i in range(len(tokens) - n + 1)):\n",
        "    ngrams[ngram] += 1\n",
        "  return ngrams\n",
        "\n",
        "\n",
        "def _score_lcs(target_tokens, prediction_tokens):\n",
        "  \"\"\"Computes LCS (Longest Common Subsequence) rouge scores.\n",
        "\n",
        "  Args:\n",
        "    target_tokens: Tokens from the target text.\n",
        "    prediction_tokens: Tokens from the predicted text.\n",
        "  Returns:\n",
        "    A Score object containing computed scores.\n",
        "  \"\"\"\n",
        "\n",
        "  if not target_tokens or not prediction_tokens:\n",
        "    return scoring.Score(precision=0, recall=0, fmeasure=0)\n",
        "\n",
        "  # Compute length of LCS from the bottom up in a table (DP appproach).\n",
        "  lcs_table = _lcs_table(target_tokens, prediction_tokens)\n",
        "  lcs_length = lcs_table[-1][-1]\n",
        "\n",
        "  precision = lcs_length / len(prediction_tokens)\n",
        "  recall = lcs_length / len(target_tokens)\n",
        "  fmeasure = scoring.fmeasure(precision, recall)\n",
        "\n",
        "  return scoring.Score(precision=precision, recall=recall, fmeasure=fmeasure)\n",
        "\n",
        "\n",
        "def _lcs_table(ref, can):\n",
        "  \"\"\"Create 2-d LCS score table.\"\"\"\n",
        "  rows = len(ref)\n",
        "  cols = len(can)\n",
        "  lcs_table = [[0] * (cols + 1) for _ in range(rows + 1)]\n",
        "  for i in range(1, rows + 1):\n",
        "    for j in range(1, cols + 1):\n",
        "      if ref[i - 1] == can[j - 1]:\n",
        "        lcs_table[i][j] = lcs_table[i - 1][j - 1] + 1\n",
        "      else:\n",
        "        lcs_table[i][j] = max(lcs_table[i - 1][j], lcs_table[i][j - 1])\n",
        "  return lcs_table\n",
        "\n",
        "\n",
        "def _backtrack_norec(t, ref, can):\n",
        "  \"\"\"Read out LCS.\"\"\"\n",
        "  i = len(ref)\n",
        "  j = len(can)\n",
        "  lcs = []\n",
        "  while i > 0 and j > 0:\n",
        "    if ref[i - 1] == can[j - 1]:\n",
        "      lcs.insert(0, i-1)\n",
        "      i -= 1\n",
        "      j -= 1\n",
        "    elif t[i][j - 1] > t[i - 1][j]:\n",
        "      j -= 1\n",
        "    else:\n",
        "      i -= 1\n",
        "  return lcs\n",
        "\n",
        "\n",
        "def _summary_level_lcs(ref_sent, can_sent):\n",
        "  \"\"\"ROUGE: Summary-level LCS, section 3.2 in ROUGE paper.\n",
        "\n",
        "  Args:\n",
        "    ref_sent: list of tokenized reference sentences\n",
        "    can_sent: list of tokenized candidate sentences\n",
        "\n",
        "  Returns:\n",
        "    summary level ROUGE score\n",
        "  \"\"\"\n",
        "  if not ref_sent or not can_sent:\n",
        "    return scoring.Score(precision=0, recall=0, fmeasure=0)\n",
        "\n",
        "  m = sum(map(len, ref_sent))\n",
        "  n = sum(map(len, can_sent))\n",
        "  if not n or not m:\n",
        "    return scoring.Score(precision=0, recall=0, fmeasure=0)\n",
        "\n",
        "  # get token counts to prevent double counting\n",
        "  token_cnts_r = collections.Counter()\n",
        "  token_cnts_c = collections.Counter()\n",
        "  for s in ref_sent:\n",
        "    # s is a list of tokens\n",
        "    token_cnts_r.update(s)\n",
        "  for s in can_sent:\n",
        "    token_cnts_c.update(s)\n",
        "\n",
        "  hits = 0\n",
        "  for r in ref_sent:\n",
        "    lcs = _union_lcs(r, can_sent)\n",
        "    # Prevent double-counting:\n",
        "    # The paper describes just computing hits += len(_union_lcs()),\n",
        "    # but the implementation prevents double counting. We also\n",
        "    # implement this as in version 1.5.5.\n",
        "    for t in lcs:\n",
        "      if token_cnts_c[t] > 0 and token_cnts_r[t] > 0:\n",
        "        hits += 1\n",
        "        token_cnts_c[t] -= 1\n",
        "        token_cnts_r[t] -= 1\n",
        "\n",
        "  recall = hits / m\n",
        "  precision = hits / n\n",
        "  fmeasure = scoring.fmeasure(precision, recall)\n",
        "  return scoring.Score(precision=precision, recall=recall, fmeasure=fmeasure)\n",
        "\n",
        "\n",
        "def _union_lcs(ref, c_list):\n",
        "  \"\"\"Find union LCS between a ref sentence and list of candidate sentences.\n",
        "\n",
        "  Args:\n",
        "    ref: list of tokens\n",
        "    c_list: list of list of indices for LCS into reference summary\n",
        "\n",
        "  Returns:\n",
        "    List of tokens in ref representing union LCS.\n",
        "  \"\"\"\n",
        "  lcs_list = [lcs_ind(ref, c) for c in c_list]\n",
        "  return [ref[i] for i in _find_union(lcs_list)]\n",
        "\n",
        "\n",
        "def _find_union(lcs_list):\n",
        "  \"\"\"Finds union LCS given a list of LCS.\"\"\"\n",
        "  return sorted(list(set().union(*lcs_list)))\n",
        "\n",
        "\n",
        "def lcs_ind(ref, can):\n",
        "  \"\"\"Returns one of the longest lcs.\"\"\"\n",
        "  t = _lcs_table(ref, can)\n",
        "  return _backtrack_norec(t, ref, can)\n",
        "\n",
        "\n",
        "def _score_ngrams(target_ngrams, prediction_ngrams):\n",
        "  \"\"\"Compute n-gram based rouge scores.\n",
        "\n",
        "  Args:\n",
        "    target_ngrams: A Counter object mapping each ngram to number of\n",
        "      occurrences for the target text.\n",
        "    prediction_ngrams: A Counter object mapping each ngram to number of\n",
        "      occurrences for the prediction text.\n",
        "  Returns:\n",
        "    A Score object containing computed scores.\n",
        "  \"\"\"\n",
        "\n",
        "  intersection_ngrams_count = 0\n",
        "  for ngram in six.iterkeys(target_ngrams):\n",
        "    intersection_ngrams_count += min(target_ngrams[ngram],\n",
        "                                     prediction_ngrams[ngram])\n",
        "  target_ngrams_count = sum(target_ngrams.values())\n",
        "  prediction_ngrams_count = sum(prediction_ngrams.values())\n",
        "\n",
        "  precision = intersection_ngrams_count / max(prediction_ngrams_count, 1)\n",
        "  recall = intersection_ngrams_count / max(target_ngrams_count, 1)\n",
        "  fmeasure = scoring.fmeasure(precision, recall)\n",
        "\n",
        "  return scoring.Score(precision=precision, recall=recall, fmeasure=fmeasure)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.19.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge-score) (3.2.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (0.12.0)\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84ffUwZc-1eQ"
      },
      "source": [
        "def tokenize(text, stemmer):\n",
        "  \"\"\"Tokenize input text into a list of tokens.\n",
        "\n",
        "  This approach aims to replicate the approach taken by Chin-Yew Lin in\n",
        "  the original ROUGE implementation.\n",
        "\n",
        "  Args:\n",
        "    text: A text blob to tokenize.\n",
        "    stemmer: An optional stemmer.\n",
        "\n",
        "  Returns:\n",
        "    A list of string tokens extracted from input text.\n",
        "  \"\"\"\n",
        "\n",
        "  # Convert everything to lowercase.\n",
        "  text = text.lower()\n",
        "  # Replace any non-alpha-numeric characters with spaces.\n",
        "  # text = re.sub(r\"[^a-z0-9]+\", \" \", six.ensure_str(text))\n",
        "  # ^ [ / u4E00 - / u9FFF]+$\n",
        "  tokens = re.split(r\"\\s+\", text)\n",
        "\n",
        "  # print(tokens)\n",
        "  # print(1)\n",
        "  if stemmer:\n",
        "    # Only stem words more than 3 characters long.\n",
        "    tokens = [stemmer.stem(x) if len(x) > 3 else x for x in tokens]\n",
        "\n",
        "  # print(tokens)\n",
        "  # print(2)\n",
        "  # print(six.ensure_str(x))\n",
        "  # One final check to drop any empty or invalid tokens.\n",
        "  # tokens = [x for x in tokens if re.match(r\"^[/u4E00 - /u9FFF]+$\", six.ensure_str(x))]\n",
        "  # tokens = [x for x in tokens if re.match(r\"^[a-z0-9]+$\", six.ensure_str(x))]\n",
        "  tokens=[x for x in tokens if six.ensure_str]\n",
        "\n",
        "\n",
        "  # print(tokens)\n",
        "  # print(3)\n",
        "  return tokens"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aasRZ__m-4qA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}