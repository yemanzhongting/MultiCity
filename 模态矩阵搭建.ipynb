{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "22_Image_Captioning.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8jJ2H-lAmga"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pickle as pkl\n",
        "import keras\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Input, BatchNormalization, \\\n",
        "    multiply, concatenate, Flatten, Activation, dot\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import plot_model\n",
        "from keras.callbacks import EarlyStopping\n",
        "import pydot as pyd\n",
        "from keras.utils.vis_utils import plot_model, model_to_dot\n",
        "keras.utils.vis_utils.pydot = pyd"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1YrYwj8EODi",
        "outputId": "c4c34e56-7e33-462a-8698-de15b6f3d482"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWpYi4EqEJmR"
      },
      "source": [
        "\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import nltk"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdA6XdVQEd9l"
      },
      "source": [
        "def load_data(in_file):\n",
        "    cn = []\n",
        "    en = []\n",
        "    num_examples = 0\n",
        "    with open(in_file, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip().split(\"\\t\")\n",
        "            \n",
        "            # en.append([\"BOS\"] + nltk.word_tokenize(line[0].lower()) + [\"EOS\"])\n",
        "            en.append(nltk.word_tokenize(line[0].lower()))\n",
        "            # split chinese sentence into characters\n",
        "            # cn.append([\"BOS\"] + [c for c in line[1].split(' ')] + [\"EOS\"])\n",
        "            cn.append([c for c in line[1].split(' ')] )\n",
        "    return en, cn"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRr2LVIsEeAz",
        "outputId": "c7ddcc13-9dc5-4f98-c704-30c11fe9f77e"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "train_file = \"/content/drive/MyDrive/data/sv-sub.txt\"\n",
        "dev_file = \"nmt/en-cn/dev.txt\"\n",
        "train_en, train_cn = load_data(train_file)\n",
        "# dev_en, dev_cn = load_data(dev_file)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIqU68rrSBjx",
        "outputId": "5f992a23-304c-48f2-f0f9-a09d657150d4"
      },
      "source": [
        "type(train_en)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItMRc7WORmXy"
      },
      "source": [
        "UNK_IDX = 0\n",
        "PAD_IDX = 1\n",
        "def build_dict(sentences, max_words=50000):\n",
        "    word_count = Counter()\n",
        "    for sentence in sentences:\n",
        "        for s in sentence:\n",
        "            word_count[s] += 1\n",
        "    ls = word_count.most_common(max_words)\n",
        "    total_words = len(ls) #+ 2\n",
        "    # word_dict = {w[0]: index+2 for index, w in enumerate(ls)}\n",
        "    word_dict = {w[0]: index+1 for index, w in enumerate(ls)}\n",
        "    # word_dict[\"UNK\"] = UNK_IDX\n",
        "    # word_dict[\"PAD\"] = PAD_IDX\n",
        "    return word_dict, total_words\n",
        "\n",
        "en_dict, en_total_words = build_dict(train_en)\n",
        "cn_dict, cn_total_words = build_dict(train_cn)\n",
        "inv_en_dict = {v: k for k, v in en_dict.items()}\n",
        "inv_cn_dict = {v: k for k, v in cn_dict.items()}"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNrpduHnJt8k"
      },
      "source": [
        "from keras.utils import np_utils\n",
        "\n",
        "def words_2_ints(words):\n",
        "    ints = []\n",
        "    for itmp in words:\n",
        "        ints.append(en_dict.get(itmp, 0))\n",
        "    return ints"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4gal5mgJyxW"
      },
      "source": [
        "def words_2_one_hot(words, num_classes=word_len):\n",
        "    return np_utils.to_categorical(words_2_ints(words), num_classes=num_classes)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1kI1hDkR4AH"
      },
      "source": [
        "import tensorflow as tf\n",
        "# from tf.keras.preprocessing.sequence import pad_sequences\n",
        "train_en_pad=[]\n",
        "for i in train_en:\n",
        "  tmp=tf.keras.preprocessing.sequence.pad_sequences(words_2_one_hot(i,len(en_dict)+1).T, padding=\"post\", maxlen=10,value=0).T\n",
        "  train_en_pad.append(tmp)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcuadE51Jy0Q",
        "outputId": "fdde2a28-abee-45e9-cafd-50e52cb68993"
      },
      "source": [
        "train_en_pad_arr=np.array(train_en_pad)\n",
        "train_en_pad_arr.shape"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20790, 10, 140)"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnO4Uu16Tk5r",
        "outputId": "0c6019c7-8049-4cb5-dd2f-8bf4c1ea75bc"
      },
      "source": [
        "from keras.utils import np_utils\n",
        "\n",
        "def words_2_ints(words):\n",
        "    ints = []\n",
        "    for itmp in words:\n",
        "        ints.append(cn_dict.get(itmp, 0))\n",
        "    return ints\n",
        "def words_2_one_hot(words, num_classes=word_len):\n",
        "    return np_utils.to_categorical(words_2_ints(words), num_classes=num_classes)\n",
        "import tensorflow as tf\n",
        "# from tf.keras.preprocessing.sequence import pad_sequences\n",
        "train_en_pad=[]\n",
        "for i in train_cn:\n",
        "  tmp=tf.keras.preprocessing.sequence.pad_sequences(words_2_one_hot(i,len(cn_dict)+1).T, padding=\"post\", maxlen=10,value=0).T\n",
        "  train_en_pad.append(tmp)\n",
        "train_cn_pad_arr=np.array(train_en_pad)\n",
        "train_cn_pad_arr.shape"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20790, 10, 155)"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtsAN96XNGD7"
      },
      "source": [
        "\n",
        "# model=Sequential()\n",
        "# model.add(LSTM(256,dropout_W=0.2,dropout_u=0.2,input_shape(seq_length,128)))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(Dense(128,activation=\"sigmoid\"))\n",
        "# model,compile(loss=\"mse\",optimizer=\"adm\")\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pickle as pkl\n",
        "import keras\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Input, BatchNormalization, \\\n",
        "    multiply, concatenate, Flatten, Activation, dot\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam # - Works\n",
        "\n",
        "# from keras.optimizers import Adam\n",
        "# from keras.utils import plot_model\n",
        "from keras.callbacks import EarlyStopping\n",
        "import pydot as pyd\n",
        "# from keras.utils.vis_utils import plot_model, model_to_dot\n",
        "keras.utils.vis_utils.pydot = pyd"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-L6vxRYG-Gc"
      },
      "source": [
        "n_hidden = 100\n",
        "input_train = Input(shape=(train_en_pad_arr.shape[1], train_en_pad_arr.shape[2]-1))\n",
        "output_train = Input(shape=(train_cn_pad_arr.shape[1], train_cn_pad_arr.shape[2]-1))"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeH6rPu3VQZP"
      },
      "source": [
        "https://levelup.gitconnected.com/building-seq2seq-lstm-with-luong-attention-in-keras-for-time-series-forecasting-1ee00958decb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmIoPA04VRqf",
        "outputId": "6c22bb52-c7bd-4a19-ff63-61148676c5f2"
      },
      "source": [
        "encoder_stack_h, encoder_last_h, encoder_last_c = LSTM(\n",
        "    n_hidden, activation='elu', dropout=0.2, recurrent_dropout=0.2, \n",
        "    return_state=True, return_sequences=True)(input_train)\n",
        "print(encoder_stack_h)\n",
        "print(encoder_last_h)\n",
        "print(encoder_last_c)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 10, 100), dtype=tf.float32, name=None), name='lstm/transpose_1:0', description=\"created by layer 'lstm'\")\n",
            "KerasTensor(type_spec=TensorSpec(shape=(None, 100), dtype=tf.float32, name=None), name='lstm/while:4', description=\"created by layer 'lstm'\")\n",
            "KerasTensor(type_spec=TensorSpec(shape=(None, 100), dtype=tf.float32, name=None), name='lstm/while:5', description=\"created by layer 'lstm'\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jx-K_xtlVcSE"
      },
      "source": [
        "encoder_last_h = BatchNormalization(momentum=0.6)(encoder_last_h)\n",
        "encoder_last_c = BatchNormalization(momentum=0.6)(encoder_last_c)"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoeqIgsvVgNP",
        "outputId": "e6f24982-1058-4e71-c9c0-9576bc7a9465"
      },
      "source": [
        "decoder_input = RepeatVector(output_train.shape[1])(encoder_last_h)\n",
        "print(decoder_input)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 10, 100), dtype=tf.float32, name=None), name='repeat_vector/Tile:0', description=\"created by layer 'repeat_vector'\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdSIRMG_ViZR",
        "outputId": "51102bae-2641-4201-debe-25e1a73e2a8d"
      },
      "source": [
        "decoder_stack_h = LSTM(n_hidden, activation='elu', dropout=0.2, recurrent_dropout=0.2,\n",
        " return_state=False, return_sequences=True)(\n",
        " decoder_input, initial_state=[encoder_last_h, encoder_last_c])\n",
        "print(decoder_stack_h)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 10, 100), dtype=tf.float32, name=None), name='lstm_1/transpose_1:0', description=\"created by layer 'lstm_1'\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RDldd8cVidj",
        "outputId": "3393812b-2a0e-40c9-ddf2-db8279a6e9e5"
      },
      "source": [
        "attention = dot([decoder_stack_h, encoder_stack_h], axes=[2, 2])\n",
        "attention = Activation('softmax')(attention)\n",
        "print(attention)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 10, 10), dtype=tf.float32, name=None), name='activation/Softmax:0', description=\"created by layer 'activation'\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCApwHtJVigr",
        "outputId": "0556767d-bbb6-40be-b93e-f369d978f717"
      },
      "source": [
        "context = dot([attention, encoder_stack_h], axes=[2,1])\n",
        "context = BatchNormalization(momentum=0.6)(context)\n",
        "print(context)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 10, 100), dtype=tf.float32, name=None), name='batch_normalization_2/batchnorm/add_1:0', description=\"created by layer 'batch_normalization_2'\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHoUzButVpIE",
        "outputId": "6048cc12-4224-4818-d8a1-d5d098e3eae9"
      },
      "source": [
        "decoder_combined_context = concatenate([context, decoder_stack_h])\n",
        "print(decoder_combined_context)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 10, 200), dtype=tf.float32, name=None), name='concatenate/concat:0', description=\"created by layer 'concatenate'\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUsoNHDmVpLD",
        "outputId": "f92a4255-9517-4fc0-999c-eb1594d7f02f"
      },
      "source": [
        "out = TimeDistributed(Dense(output_train.shape[2]))(decoder_combined_context)\n",
        "print(out)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 10, 154), dtype=tf.float32, name=None), name='time_distributed/Reshape_1:0', description=\"created by layer 'time_distributed'\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3Y7topTVpQS",
        "outputId": "7a5dbfb4-8956-44e6-b756-9bb36085a4d0"
      },
      "source": [
        "model = Model(inputs=input_train, outputs=out)\n",
        "opt = Adam(lr=0.01, clipnorm=1)\n",
        "model.compile(loss='mean_squared_error', optimizer=opt, metrics=['mae'])\n",
        "model.summary()"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 10, 139)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 10, 100), (N 96000       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 100)          400         lstm[0][1]                       \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector (RepeatVector)    (None, 10, 100)      0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 100)          400         lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 10, 100)      80400       repeat_vector[0][0]              \n",
            "                                                                 batch_normalization[0][0]        \n",
            "                                                                 batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dot (Dot)                       (None, 10, 10)       0           lstm_1[0][0]                     \n",
            "                                                                 lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 10, 10)       0           dot[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, 10, 100)      0           activation[0][0]                 \n",
            "                                                                 lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 10, 100)      400         dot_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 10, 200)      0           batch_normalization_2[0][0]      \n",
            "                                                                 lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, 10, 154)      30954       concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 208,554\n",
            "Trainable params: 207,954\n",
            "Non-trainable params: 600\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6A_HLHKVijT"
      },
      "source": [
        "epc = 100\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', patience=50)\n",
        "history = model.fit(X_input_train[:, :, :2], X_output_train[:, :, :2], validation_split=0.2, \n",
        "                    epochs=epc, verbose=1, callbacks=[es], \n",
        "                    batch_size=100)\n",
        "train_mae = history.history['mae']\n",
        "valid_mae = history.history['val_mae']\n",
        " \n",
        "model.save('model_forecasting_seq2seq.h5')"
      ],
      "execution_count": 108,
      "outputs": []
    }
  ]
}