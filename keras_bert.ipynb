{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_bert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yemanzhongting/MultiCity/blob/main/keras_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGVU8dNXXIja"
      },
      "source": [
        "# English-to-Spanish translation with a sequence-to-sequence Transformer\n",
        "\n",
        "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
        "**Date created:** 2021/05/26<br>\n",
        "**Last modified:** 2021/05/26<br>\n",
        "**Description:** Implementing a sequence-to-sequene Transformer and training it on a machine translation task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XUbMP54XIjp"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this example, we'll build a sequence-to-sequence Transformer model, which\n",
        "we'll train on an English-to-Spanish machine translation task.\n",
        "\n",
        "You'll learn how to:\n",
        "\n",
        "- Vectorize text using the Keras `TextVectorization` layer.\n",
        "- Implement a `TransformerEncoder` layer, a `TransformerDecoder` layer,\n",
        "and a `PositionalEmbedding` layer.\n",
        "- Prepare data for training a sequence-to-sequence model.\n",
        "- Use the trained model to generate translations of never-seen-before\n",
        "input sentences (sequence-to-sequence inference).\n",
        "\n",
        "The code featured here is adapted from the book\n",
        "[Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition)\n",
        "(chapter 11: Deep learning for text).\n",
        "The present example is fairly barebones, so for detailed explanations of\n",
        "how each building block works, as well as the theory behind Transformers,\n",
        "I recommend reading the book."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XC2uSopXIjr"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TTXK3grXIjs"
      },
      "source": [
        "import pathlib\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization"
      ],
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFn0YhSQDomc",
        "outputId": "b8e72e13-ee4e-4fcf-c6d1-113f4749fb03"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECK0SY71XIjv"
      },
      "source": [
        "## Downloading the data\n",
        "\n",
        "We'll be working with an English-to-Spanish translation dataset\n",
        "provided by [Anki](https://www.manythings.org/anki/). Let's download it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lkjhZ4dXIjw"
      },
      "source": [
        "text_file = keras.utils.get_file(\n",
        "    fname=\"spa-eng.zip\",\n",
        "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
        "    extract=True,\n",
        ")\n",
        "text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""
      ],
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNx7iSnsD0s7",
        "outputId": "61472493-c5b0-41f6-e67c-b9d7d6e7f2ff"
      },
      "source": [
        "text_file"
      ],
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/root/.keras/datasets/spa-eng/spa.txt')"
            ]
          },
          "metadata": {},
          "execution_count": 236
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7-h3V-3XIjy"
      },
      "source": [
        "## Parsing the data\n",
        "\n",
        "Each line contains an English sentence and its corresponding Spanish sentence.\n",
        "The English sentence is the *source sequence* and Spanish one is the *target sequence*.\n",
        "We prepend the token `\"[start]\"` and we append the token `\"[end]\"` to the Spanish sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ghezIbKXIjz"
      },
      "source": [
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    eng, spa = line.split(\"\\t\")\n",
        "    spa = \"[start] \" + spa + \" [end]\"\n",
        "    text_pairs.append((eng, spa))"
      ],
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bivBRB7oEDxe"
      },
      "source": [
        "with open('/content/drive/MyDrive/data/sv-sub.txt') as f:\n",
        "    lines = f.readlines()\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    eng, spa = line.strip().split(\"\\t\")\n",
        "    spa = \"[start] \" + spa + \" [end]\"\n",
        "    text_pairs.append((eng, spa))"
      ],
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-PD1ZpfluNb",
        "outputId": "ee38daa3-a55c-4e08-a5aa-9517c0bc85f0"
      },
      "source": [
        "text_pairs[0]"
      ],
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Tree Plant House Window Car Land Street Billboard Tire',\n",
              " '[start] 物流速递 住宅区 美容美发店 公司 中餐厅 诊所 政府机关 福特特约维修 临街院门 交通地名 [end]')"
            ]
          },
          "metadata": {},
          "execution_count": 239
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ap7uXms1lxdI",
        "outputId": "97cd568b-c3fc-49aa-dd35-cb502906b0cb"
      },
      "source": [
        "text_pairs[0]"
      ],
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Tree Plant House Window Car Land Street Billboard Tire',\n",
              " '[start] 物流速递 住宅区 美容美发店 公司 中餐厅 诊所 政府机关 福特特约维修 临街院门 交通地名 [end]')"
            ]
          },
          "metadata": {},
          "execution_count": 240
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_fbUyM2XIj4"
      },
      "source": [
        "Here's what our sentence pairs look like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-WuGFfrXIj6",
        "outputId": "67cc9539-72bb-4388-9cbc-dd3e76c4139f"
      },
      "source": [
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs))"
      ],
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Tree Plant Window Car Wheel House Building Skyscraper Street Vehicle', '[start] 中餐厅 大众特约销售 家居建材市场 公交车站 生活服务场所 停车场 临街院门 银行 医药保健销售店 自动提款机 [end]')\n",
            "('Building Window Plant Car Wheel Skyscraper Tree Street House Land', '[start] 娱乐场所 中餐厅 美容美发店 生活服务场所 临街院门 物流速递 宾馆酒店 公检法机构 餐饮相关场所 汽车养护/装饰 [end]')\n",
            "('Tree Window Wheel Plant Car Building Land Skyscraper Street Tower', '[start] 临街院门 汽车配件销售 住宅区 中餐厅 政府机关 学校 交通地名 培训机构 科教文化场所 停车场 [end]')\n",
            "('Tree Building Window Car Wheel Street Land Vehicle Skyscraper Tire', '[start] 停车场 农林牧渔基地 住宅区 自动提款机 学校 宾馆酒店 公司 洗浴推拿场所 临街院门 旅馆招待所 [end]')\n",
            "('Tree Plant Building House Car Window', '[start] 临街院门 公司 停车场 农林牧渔基地 生活服务场所 宾馆酒店 福特特约维修 产业园区 商务住宅相关 物流速递 [end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdT9O0McXIj8"
      },
      "source": [
        "Now, let's split the sentence pairs into a training set, a validation set,\n",
        "and a test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8G0_QeEHXIj9",
        "outputId": "107702e4-5624-4242-c3ad-299c3c7b79e8"
      },
      "source": [
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:]\n",
        "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")"
      ],
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20790 total pairs\n",
            "20790 training pairs\n",
            "3118 validation pairs\n",
            "3118 test pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5Vw2VZ9pypT",
        "outputId": "3ebce5b2-9d55-4d43-b48b-0e347b34e3e3"
      },
      "source": [
        "test_pairs[0]"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Tree Window Person Building Car Wheel Bus Land Footwear Truck',\n",
              " '[start] 停车场 中餐厅 科教文化场所 生活服务场所 传媒机构 农林牧渔基地 楼宇 临街院门 冷饮店 宾馆酒店 [end]')"
            ]
          },
          "metadata": {},
          "execution_count": 243
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iQUj0ZwXIj-"
      },
      "source": [
        "## Vectorizing the text data\n",
        "\n",
        "We'll use two instances of the `TextVectorization` layer to vectorize the text\n",
        "data (one for English and one for Spanish),\n",
        "that is to say, to turn the original strings into integer sequences\n",
        "where each integer represents the index of a word in a vocabulary.\n",
        "\n",
        "The English layer will use the default string standardization (strip punctuation characters)\n",
        "and splitting scheme (split on whitespace), while\n",
        "the Spanish layer will use a custom standardization, where we add the character\n",
        "`\"¿\"` to the set of punctuation characters to be stripped.\n",
        "\n",
        "Note: in a production-grade machine translation model, I would not recommend\n",
        "stripping the punctuation characters in either language. Instead, I would recommend turning\n",
        "each punctuation character into its own token,\n",
        "which you could achieve by providing a custom `split` function to the `TextVectorization` layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx9BouLYl449"
      },
      "source": [
        "##对文本数据进行矢量化\n",
        "\n",
        "我们将使用 \"TextVectorization \"层的两个实例来对文本数据进行矢量化。\n",
        "数据（一个用于英语，一个用于西班牙语）。\n",
        "也就是说，将原始字符串变成整数序列\n",
        "其中每个整数代表词汇表中的一个词的索引。\n",
        "\n",
        "英语层将使用默认的字符串标准化（去除标点符号\n",
        "和分割方案（在空白处分割），而\n",
        "而西班牙语层将使用一个自定义的标准化，我们在其中添加字符\n",
        "`\"¿\"`到要剥离的标点符号集合中。\n",
        "\n",
        "注意：在一个生产级的机器翻译模型中，我不建议\n",
        "剥离两种语言中的标点符号。相反，我建议将\n",
        "把每个标点符号变成自己的标记。\n",
        "你可以通过为 \"文本矢量化 \"层提供一个自定义的 \"分割 \"函数来实现。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWVMvv0vXIkA"
      },
      "source": [
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "vocab_size = 200\n",
        "sequence_length = 10\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "\n",
        "eng_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n",
        ")\n",
        "spa_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_eng_texts = [pair[0] for pair in train_pairs]\n",
        "train_spa_texts = [pair[1] for pair in train_pairs]\n",
        "eng_vectorization.adapt(train_eng_texts)\n",
        "spa_vectorization.adapt(train_spa_texts)"
      ],
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Z9CtmW1NmHMB",
        "outputId": "32bbea68-5b93-41da-e4e1-ac06be8b9beb"
      },
      "source": [
        "train_spa_texts[0]"
      ],
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[start] 公司 中餐厅 生活服务场所 临街院门 医药保健销售店 学校 政府机关 餐饮相关场所 快餐厅 婴儿服务场所 [end]'"
            ]
          },
          "metadata": {},
          "execution_count": 245
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pQUQ7l-zmBSw",
        "outputId": "3dee8bef-9014-4185-fe1d-423969d43cbf"
      },
      "source": [
        "train_eng_texts[0]"
      ],
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tree Window House Wheel Car Footwear Land Person Tire Flower'"
            ]
          },
          "metadata": {},
          "execution_count": 246
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hupurzzImLcu"
      },
      "source": [
        "eng_vectorization.adapt(train_eng_texts)"
      ],
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImdXD4umXIkD"
      },
      "source": [
        "Next, we'll format our datasets.\n",
        "\n",
        "At each training step, the model will seek to predict target words N+1 (and beyond)\n",
        "using the source sentence and the target words 0 to N.\n",
        "\n",
        "As such, the training dataset will yield a tuple `(inputs, targets)`, where:\n",
        "\n",
        "- `inputs` is a dictionary with the keys `encoder_inputs` and `decoder_inputs`.\n",
        "`encoder_inputs` is the vectorized source sentence and `encoder_inputs` is the target sentence \"so far\",\n",
        "that is to say, the words 0 to N used to predict word N+1 (and beyond) in the target sentence.\n",
        "- `target` is the target sentence offset by one step:\n",
        "it provides the next words in the target sentence -- what the model will try to predict."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVu-jP9Im8nb",
        "outputId": "f60f6095-a570-4069-a565-82e90ed4f121"
      },
      "source": [
        "eng_vectorization(\"Tree Plant Car Land Street Flower House\")"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=int64, numpy=array([ 2,  8,  5, 10, 11, 17,  7,  0,  0,  0])>"
            ]
          },
          "metadata": {},
          "execution_count": 248
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iz-5jsKCnDIj",
        "outputId": "266d9043-8b7c-4833-f7ef-0f7bd09a01d9"
      },
      "source": [
        "spa_vectorization('[start] 中餐厅 生活服务场所 临街院门 停车场 风景名胜 培训机构 公司 娱乐场所 商务住宅相关 售票处 [end]')"
      ],
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(11,), dtype=int64, numpy=array([ 2,  6,  4,  5,  8, 40, 18, 10, 17, 54, 61])>"
            ]
          },
          "metadata": {},
          "execution_count": 249
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljMyXtdimaRg"
      },
      "source": [
        "def format_dataset(eng, spa):\n",
        "    eng = eng_vectorization(eng)\n",
        "    spa = spa_vectorization(spa)\n",
        "    return ({\"encoder_inputs\": eng, \"decoder_inputs\": spa[:, :-1],}, spa[:, 1:])"
      ],
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P84fFoIumc8A"
      },
      "source": [
        "# format_dataset(\"You don't seem very concerned.\", 'No pareces muy preocupado.')"
      ],
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqZiUFxTXIkF"
      },
      "source": [
        "def make_dataset(pairs):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVtOD97jmSRy",
        "outputId": "ed8caa61-0a95-48fe-a80e-a5472c42fe70"
      },
      "source": [
        "train_ds"
      ],
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<CacheDataset shapes: ({encoder_inputs: (None, 10), decoder_inputs: (None, 10)}, (None, 10)), types: ({encoder_inputs: tf.int64, decoder_inputs: tf.int64}, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 253
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLCX8kWAXIkG"
      },
      "source": [
        "Let's take a quick look at the sequence shapes\n",
        "(we have batches of 64 pairs, and all sequences are 20 steps long):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3JnZ_HeXIkH",
        "outputId": "6390a823-76ec-41a7-81ea-1ba5b4c750dc"
      },
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ],
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs[\"encoder_inputs\"].shape: (64, 10)\n",
            "inputs[\"decoder_inputs\"].shape: (64, 10)\n",
            "targets.shape: (64, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vETfD2-WXIkI"
      },
      "source": [
        "## Building the model\n",
        "\n",
        "Our sequence-to-sequence Transformer consists of a `TransformerEncoder`\n",
        "and a `TransformerDecoder` chained together. To make the model aware of word order,\n",
        "we also use a `PositionalEmbedding` layer.\n",
        "\n",
        "The source sequence will be pass to the `TransformerEncoder`,\n",
        "which will produce a new representation of it.\n",
        "This new representation will then be passed\n",
        "to the `TransformerDecoder`, together with the target sequence so far (target words 0 to N).\n",
        "The `TransformerDecoder` will then seek to predict the next words in the target sequence (N+1 and beyond).\n",
        "\n",
        "A key detail that makes this possible is causal masking\n",
        "(see method `get_causal_attention_mask()` on the `TransformerDecoder`).\n",
        "The `TransformerDecoder` sees the entire sequences at once, and thus we must make\n",
        "sure that it only uses information from target tokens 0 to N when predicting token N+1\n",
        "(otherwise, it could use information from the future, which would\n",
        "result in a model that cannot be used at inference time)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pdnFAxitp6j"
      },
      "source": [
        "## 构建模型\n",
        "\n",
        "我们的序列到序列的转换器由一个 \"TransformerEncoder \"和一个 \"TransformerDecoder \"组成。\n",
        "和一个 \"TransformerDecoder \"链在一起。为了使模型能够意识到词的顺序。\n",
        "我们还使用了一个 \"位置嵌入 \"层。\n",
        "\n",
        "源序列将被传递给 \"转化器编码器\"。\n",
        "它将产生一个新的表示。\n",
        "然后这个新的表示将被传递给\n",
        "到 \"TransformerDecoder\"，连同到目前为止的目标序列（目标字0到N）。\n",
        "然后`TransformerDecoder'将寻求预测目标序列中的下一个词（N+1及以后）。\n",
        "\n",
        "使之成为可能的一个关键细节是因果掩码\n",
        "(见`TransformerDecoder'上的`get_causal_attention_mask()'方法)。\n",
        "变换器解码器 \"一次就能看到整个序列，因此我们必须确保它只使用目标信息。\n",
        "因此我们必须确保它在预测N+1号标记时只使用目标标记0到N的信息。\n",
        "(的信息（否则，它可能使用未来的信息，这将导致\n",
        "否则，它可能会使用未来的信息，这将导致在推理时不能使用的模型）。)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YT6V2OHXIkJ"
      },
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super(TransformerEncoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
        "        )\n",
        "\n",
        "        # print(attention_output)\n",
        "\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output),attention_output\n",
        "\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super(TransformerDecoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n"
      ],
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1EIjkKvXIkK"
      },
      "source": [
        "Next, we assemble the end-to-end model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQrmskp5XIkL"
      },
      "source": [
        "embed_dim = 128#256\n",
        "latent_dim = 1024#2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)"
      ],
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO91SV_JutJK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d88d9c45-5cd9-4492-f4db-35792fb6a39f"
      },
      "source": [
        "TransformerEncoder(embed_dim, latent_dim, num_heads)(x)"
      ],
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<KerasTensor: shape=(None, None, 128) dtype=float32 (created by layer 'transformer_encoder_12')>,\n",
              " <KerasTensor: shape=(None, None, 128) dtype=float32 (created by layer 'transformer_encoder_12')>)"
            ]
          },
          "metadata": {},
          "execution_count": 257
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TamtUrpRupxP"
      },
      "source": [
        "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)[0]\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "transformer = keras.Model(\n",
        "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
        ")"
      ],
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uugXdDwDXIkM"
      },
      "source": [
        "## Training our model\n",
        "\n",
        "We'll use accuracy as a quick way to monitor training progress on the validation data.\n",
        "Note that machine translation typically uses BLEU scores as well as other metrics, rather than accuracy.\n",
        "\n",
        "Here we only train for 1 epoch, but to get the model to actually converge\n",
        "you should train for at least 30 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "ItCylotwa2zp",
        "outputId": "8de8f7a6-8216-4002-b869-87545f78d1fe"
      },
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "tf.keras.utils.plot_model(\n",
        "transformer, to_file='model.png'\n",
        ")\n",
        "# , show_shapes=False, show_dtype=False,\n",
        "# show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96\n",
        "# plot_model(transformer, to_file='model.png')"
      ],
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 259
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_ioQXx-XIkN",
        "outputId": "82b02bab-361d-40d0-996a-8e4578086cb1"
      },
      "source": [
        "epochs = 1  # This should be at least 30 for convergence\n",
        "\n",
        "transformer.summary()\n",
        "transformer.compile(\n",
        "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "transformer.fit(train_ds, epochs=epochs)#, validation_data=val_ds"
      ],
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "positional_embedding_13 (Positi (None, None, 128)    26880       encoder_inputs[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "transformer_encoder_13 (Transfo ((None, None, 128),  1318656     positional_embedding_13[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "model_13 (Functional)           (None, None, 200)    2426440     decoder_inputs[0][0]             \n",
            "                                                                 transformer_encoder_13[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 3,771,976\n",
            "Trainable params: 3,771,976\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "325/325 [==============================] - 23s 55ms/step - loss: 3.5668 - accuracy: 0.1457\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe8f8393450>"
            ]
          },
          "metadata": {},
          "execution_count": 260
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkpBMxC7o7vo",
        "outputId": "8ee40e37-fcd8-4f85-d072-73d38c971596"
      },
      "source": [
        "epochs = 20  # This should be at least 30 for convergence\n",
        "\n",
        "transformer.summary()\n",
        "transformer.compile(\n",
        "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)#"
      ],
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "positional_embedding_13 (Positi (None, None, 128)    26880       encoder_inputs[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "transformer_encoder_13 (Transfo ((None, None, 128),  1318656     positional_embedding_13[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "model_13 (Functional)           (None, None, 200)    2426440     decoder_inputs[0][0]             \n",
            "                                                                 transformer_encoder_13[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 3,771,976\n",
            "Trainable params: 3,771,976\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/20\n",
            "325/325 [==============================] - 24s 61ms/step - loss: 3.2903 - accuracy: 0.1753 - val_loss: 3.1220 - val_accuracy: 0.1989\n",
            "Epoch 2/20\n",
            "325/325 [==============================] - 19s 58ms/step - loss: 3.2198 - accuracy: 0.1844 - val_loss: 3.0672 - val_accuracy: 0.2081\n",
            "Epoch 3/20\n",
            "325/325 [==============================] - 19s 58ms/step - loss: 3.1762 - accuracy: 0.1910 - val_loss: 3.0289 - val_accuracy: 0.2162\n",
            "Epoch 4/20\n",
            "325/325 [==============================] - 19s 58ms/step - loss: 3.1379 - accuracy: 0.1975 - val_loss: 2.9836 - val_accuracy: 0.2234\n",
            "Epoch 5/20\n",
            "325/325 [==============================] - 19s 58ms/step - loss: 3.1008 - accuracy: 0.2046 - val_loss: 2.9410 - val_accuracy: 0.2311\n",
            "Epoch 6/20\n",
            "325/325 [==============================] - 19s 58ms/step - loss: 3.0668 - accuracy: 0.2110 - val_loss: 2.9067 - val_accuracy: 0.2375\n",
            "Epoch 7/20\n",
            "325/325 [==============================] - 19s 58ms/step - loss: 3.0325 - accuracy: 0.2164 - val_loss: 2.8649 - val_accuracy: 0.2450\n",
            "Epoch 8/20\n",
            "325/325 [==============================] - 19s 58ms/step - loss: 2.9961 - accuracy: 0.2249 - val_loss: 2.8260 - val_accuracy: 0.2532\n",
            "Epoch 9/20\n",
            "325/325 [==============================] - 19s 58ms/step - loss: 2.9643 - accuracy: 0.2308 - val_loss: 2.7904 - val_accuracy: 0.2607\n",
            "Epoch 10/20\n",
            "325/325 [==============================] - 19s 58ms/step - loss: 2.9308 - accuracy: 0.2378 - val_loss: 2.7489 - val_accuracy: 0.2682\n",
            "Epoch 11/20\n",
            "325/325 [==============================] - 19s 58ms/step - loss: 2.8959 - accuracy: 0.2452 - val_loss: 2.7144 - val_accuracy: 0.2775\n",
            "Epoch 12/20\n",
            "325/325 [==============================] - 19s 58ms/step - loss: 2.8633 - accuracy: 0.2526 - val_loss: 2.6780 - val_accuracy: 0.2867\n",
            "Epoch 13/20\n",
            "325/325 [==============================] - 19s 58ms/step - loss: 2.8276 - accuracy: 0.2596 - val_loss: 2.6286 - val_accuracy: 0.2988\n",
            "Epoch 14/20\n",
            "325/325 [==============================] - 19s 58ms/step - loss: 2.7928 - accuracy: 0.2679 - val_loss: 2.5902 - val_accuracy: 0.3075\n",
            "Epoch 15/20\n",
            "325/325 [==============================] - 19s 58ms/step - loss: 2.7588 - accuracy: 0.2756 - val_loss: 2.5544 - val_accuracy: 0.3149\n",
            "Epoch 16/20\n",
            "325/325 [==============================] - 19s 58ms/step - loss: 2.7220 - accuracy: 0.2841 - val_loss: 2.5226 - val_accuracy: 0.3231\n",
            "Epoch 17/20\n",
            "325/325 [==============================] - 19s 58ms/step - loss: 2.6866 - accuracy: 0.2927 - val_loss: 2.4840 - val_accuracy: 0.3292\n",
            "Epoch 18/20\n",
            "325/325 [==============================] - 19s 58ms/step - loss: 2.6493 - accuracy: 0.3006 - val_loss: 2.4518 - val_accuracy: 0.3361\n",
            "Epoch 19/20\n",
            "325/325 [==============================] - 19s 58ms/step - loss: 2.6130 - accuracy: 0.3090 - val_loss: 2.3966 - val_accuracy: 0.3535\n",
            "Epoch 20/20\n",
            "325/325 [==============================] - 19s 58ms/step - loss: 2.5749 - accuracy: 0.3180 - val_loss: 2.3648 - val_accuracy: 0.3602\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe889690c90>"
            ]
          },
          "metadata": {},
          "execution_count": 261
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxrX-2yhXIkN"
      },
      "source": [
        "## Decoding test sentences\n",
        "\n",
        "Finally, let's demonstrate how to translate brand new English sentences.\n",
        "We simply feed into the model the vectorized English sentence\n",
        "as well as the target token `\"[start]\"`, then we repeatedly generated the next token, until\n",
        "we hit the token `\"[end]\"`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQ5exr2AnoLr"
      },
      "source": [
        "spa_vocab = spa_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 10"
      ],
      "execution_count": 262,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHzdpwCrnoyA"
      },
      "source": [
        "# spa_index_lookup"
      ],
      "execution_count": 263,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPgHsM2zQl4n"
      },
      "source": [
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "\n",
        "        # print(predictions)\n",
        "        # print(predictions.shape)\n",
        "\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "\n",
        "        # print(sampled_token_index)\n",
        "\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence"
      ],
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AKWczO2n_Uq",
        "outputId": "e1f471d6-c4a3-4a90-af22-815846a918bc"
      },
      "source": [
        "test_pairs[0]"
      ],
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Tree Window Person Building Car Wheel Bus Land Footwear Truck',\n",
              " '[start] 停车场 中餐厅 科教文化场所 生活服务场所 传媒机构 农林牧渔基地 楼宇 临街院门 冷饮店 宾馆酒店 [end]')"
            ]
          },
          "metadata": {},
          "execution_count": 265
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4Zvdd2dlpwl",
        "outputId": "1a2e5ccb-ca8b-49b4-c3b5-03a1eaddd188"
      },
      "source": [
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(1):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    translated = decode_sequence(input_sentence)\n",
        "    print(translated)"
      ],
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[start] 中餐厅 生活服务场所 临街院门 住宅区 美容美发店 医药保健销售店 物流速递 洗浴推拿场所 物流速递 餐饮相关场所\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjOmBi1pQsYB"
      },
      "source": [
        ""
      ],
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61WSvWseQ43x",
        "outputId": "8aa055fc-6d08-427a-f64e-ad9c4969b9e4"
      },
      "source": [
        "!pip install rouge-score\n",
        "from rouge_score import rouge_scorer\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import re\n",
        "\n",
        "from nltk.stem import porter\n",
        "import six\n",
        "from six.moves import map\n",
        "from six.moves import range\n",
        "from rouge_score import scoring\n",
        "# from rouge_score import tokenize\n",
        "\n",
        "\n",
        "class RougeScorer(scoring.BaseScorer):\n",
        "  \"\"\"Calculate rouges scores between two blobs of text.\n",
        "\n",
        "  Sample usage:\n",
        "    scorer = RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "    scores = scorer.score('The quick brown fox jumps over the lazy dog',\n",
        "                          'The quick brown dog jumps on the log.')\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, rouge_types, use_stemmer=False):\n",
        "    \"\"\"Initializes a new RougeScorer.\n",
        "\n",
        "    Valid rouge types that can be computed are:\n",
        "      rougen (e.g. rouge1, rouge2): n-gram based scoring.\n",
        "      rougeL: Longest common subsequence based scoring.\n",
        "\n",
        "    Args:\n",
        "      rouge_types: A list of rouge types to calculate.\n",
        "      use_stemmer: Bool indicating whether Porter stemmer should be used to\n",
        "        strip word suffixes to improve matching.\n",
        "    Returns:\n",
        "      A dict mapping rouge types to Score tuples.\n",
        "    \"\"\"\n",
        "\n",
        "    self.rouge_types = rouge_types\n",
        "    self._stemmer = porter.PorterStemmer() if use_stemmer else None\n",
        "\n",
        "  def score(self, target, prediction):\n",
        "    \"\"\"Calculates rouge scores between the target and prediction.\n",
        "\n",
        "    Args:\n",
        "      target: Text containing the target (ground truth) text.\n",
        "      prediction: Text containing the predicted text.\n",
        "    Returns:\n",
        "      A dict mapping each rouge type to a Score object.\n",
        "    Raises:\n",
        "      ValueError: If an invalid rouge type is encountered.\n",
        "    \"\"\"\n",
        "\n",
        "    target_tokens = tokenize(target, self._stemmer)\n",
        "    prediction_tokens = tokenize(prediction, self._stemmer)\n",
        "    result = {}\n",
        "\n",
        "    for rouge_type in self.rouge_types:\n",
        "      if rouge_type == \"rougeL\":\n",
        "        # Rouge from longest common subsequences.\n",
        "        scores = _score_lcs(target_tokens, prediction_tokens)\n",
        "      elif rouge_type == \"rougeLsum\":\n",
        "        # Note: Does not support multi-line text.\n",
        "        def get_sents(text):\n",
        "          # Assume sentences are separated by newline.\n",
        "          sents = six.ensure_str(text).split(\"\\n\")\n",
        "          sents = [x for x in sents if len(x)]\n",
        "          return sents\n",
        "\n",
        "        target_tokens_list = [\n",
        "            tokenize(s, self._stemmer) for s in get_sents(target)]\n",
        "        prediction_tokens_list = [\n",
        "            tokenize(s, self._stemmer) for s in get_sents(prediction)]\n",
        "        scores = _summary_level_lcs(target_tokens_list,\n",
        "                                    prediction_tokens_list)\n",
        "      elif re.match(r\"rouge[0-9]$\", six.ensure_str(rouge_type)):\n",
        "        # Rouge from n-grams.\n",
        "        n = int(rouge_type[5:])\n",
        "        if n <= 0:\n",
        "          raise ValueError(\"rougen requires positive n: %s\" % rouge_type)\n",
        "        target_ngrams = _create_ngrams(target_tokens, n)\n",
        "        prediction_ngrams = _create_ngrams(prediction_tokens, n)\n",
        "        scores = _score_ngrams(target_ngrams, prediction_ngrams)\n",
        "      else:\n",
        "        raise ValueError(\"Invalid rouge type: %s\" % rouge_type)\n",
        "      result[rouge_type] = scores\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def _create_ngrams(tokens, n):\n",
        "  \"\"\"Creates ngrams from the given list of tokens.\n",
        "\n",
        "  Args:\n",
        "    tokens: A list of tokens from which ngrams are created.\n",
        "    n: Number of tokens to use, e.g. 2 for bigrams.\n",
        "  Returns:\n",
        "    A dictionary mapping each bigram to the number of occurrences.\n",
        "  \"\"\"\n",
        "\n",
        "  ngrams = collections.Counter()\n",
        "  for ngram in (tuple(tokens[i:i + n]) for i in range(len(tokens) - n + 1)):\n",
        "    ngrams[ngram] += 1\n",
        "  return ngrams\n",
        "\n",
        "\n",
        "def _score_lcs(target_tokens, prediction_tokens):\n",
        "  \"\"\"Computes LCS (Longest Common Subsequence) rouge scores.\n",
        "\n",
        "  Args:\n",
        "    target_tokens: Tokens from the target text.\n",
        "    prediction_tokens: Tokens from the predicted text.\n",
        "  Returns:\n",
        "    A Score object containing computed scores.\n",
        "  \"\"\"\n",
        "\n",
        "  if not target_tokens or not prediction_tokens:\n",
        "    return scoring.Score(precision=0, recall=0, fmeasure=0)\n",
        "\n",
        "  # Compute length of LCS from the bottom up in a table (DP appproach).\n",
        "  lcs_table = _lcs_table(target_tokens, prediction_tokens)\n",
        "  lcs_length = lcs_table[-1][-1]\n",
        "\n",
        "  precision = lcs_length / len(prediction_tokens)\n",
        "  recall = lcs_length / len(target_tokens)\n",
        "  fmeasure = scoring.fmeasure(precision, recall)\n",
        "\n",
        "  return scoring.Score(precision=precision, recall=recall, fmeasure=fmeasure)\n",
        "\n",
        "\n",
        "def _lcs_table(ref, can):\n",
        "  \"\"\"Create 2-d LCS score table.\"\"\"\n",
        "  rows = len(ref)\n",
        "  cols = len(can)\n",
        "  lcs_table = [[0] * (cols + 1) for _ in range(rows + 1)]\n",
        "  for i in range(1, rows + 1):\n",
        "    for j in range(1, cols + 1):\n",
        "      if ref[i - 1] == can[j - 1]:\n",
        "        lcs_table[i][j] = lcs_table[i - 1][j - 1] + 1\n",
        "      else:\n",
        "        lcs_table[i][j] = max(lcs_table[i - 1][j], lcs_table[i][j - 1])\n",
        "  return lcs_table\n",
        "\n",
        "\n",
        "def _backtrack_norec(t, ref, can):\n",
        "  \"\"\"Read out LCS.\"\"\"\n",
        "  i = len(ref)\n",
        "  j = len(can)\n",
        "  lcs = []\n",
        "  while i > 0 and j > 0:\n",
        "    if ref[i - 1] == can[j - 1]:\n",
        "      lcs.insert(0, i-1)\n",
        "      i -= 1\n",
        "      j -= 1\n",
        "    elif t[i][j - 1] > t[i - 1][j]:\n",
        "      j -= 1\n",
        "    else:\n",
        "      i -= 1\n",
        "  return lcs\n",
        "\n",
        "\n",
        "def _summary_level_lcs(ref_sent, can_sent):\n",
        "  \"\"\"ROUGE: Summary-level LCS, section 3.2 in ROUGE paper.\n",
        "\n",
        "  Args:\n",
        "    ref_sent: list of tokenized reference sentences\n",
        "    can_sent: list of tokenized candidate sentences\n",
        "\n",
        "  Returns:\n",
        "    summary level ROUGE score\n",
        "  \"\"\"\n",
        "  if not ref_sent or not can_sent:\n",
        "    return scoring.Score(precision=0, recall=0, fmeasure=0)\n",
        "\n",
        "  m = sum(map(len, ref_sent))\n",
        "  n = sum(map(len, can_sent))\n",
        "  if not n or not m:\n",
        "    return scoring.Score(precision=0, recall=0, fmeasure=0)\n",
        "\n",
        "  # get token counts to prevent double counting\n",
        "  token_cnts_r = collections.Counter()\n",
        "  token_cnts_c = collections.Counter()\n",
        "  for s in ref_sent:\n",
        "    # s is a list of tokens\n",
        "    token_cnts_r.update(s)\n",
        "  for s in can_sent:\n",
        "    token_cnts_c.update(s)\n",
        "\n",
        "  hits = 0\n",
        "  for r in ref_sent:\n",
        "    lcs = _union_lcs(r, can_sent)\n",
        "    # Prevent double-counting:\n",
        "    # The paper describes just computing hits += len(_union_lcs()),\n",
        "    # but the implementation prevents double counting. We also\n",
        "    # implement this as in version 1.5.5.\n",
        "    for t in lcs:\n",
        "      if token_cnts_c[t] > 0 and token_cnts_r[t] > 0:\n",
        "        hits += 1\n",
        "        token_cnts_c[t] -= 1\n",
        "        token_cnts_r[t] -= 1\n",
        "\n",
        "  recall = hits / m\n",
        "  precision = hits / n\n",
        "  fmeasure = scoring.fmeasure(precision, recall)\n",
        "  return scoring.Score(precision=precision, recall=recall, fmeasure=fmeasure)\n",
        "\n",
        "\n",
        "def _union_lcs(ref, c_list):\n",
        "  \"\"\"Find union LCS between a ref sentence and list of candidate sentences.\n",
        "\n",
        "  Args:\n",
        "    ref: list of tokens\n",
        "    c_list: list of list of indices for LCS into reference summary\n",
        "\n",
        "  Returns:\n",
        "    List of tokens in ref representing union LCS.\n",
        "  \"\"\"\n",
        "  lcs_list = [lcs_ind(ref, c) for c in c_list]\n",
        "  return [ref[i] for i in _find_union(lcs_list)]\n",
        "\n",
        "\n",
        "def _find_union(lcs_list):\n",
        "  \"\"\"Finds union LCS given a list of LCS.\"\"\"\n",
        "  return sorted(list(set().union(*lcs_list)))\n",
        "\n",
        "\n",
        "def lcs_ind(ref, can):\n",
        "  \"\"\"Returns one of the longest lcs.\"\"\"\n",
        "  t = _lcs_table(ref, can)\n",
        "  return _backtrack_norec(t, ref, can)\n",
        "\n",
        "\n",
        "def _score_ngrams(target_ngrams, prediction_ngrams):\n",
        "  \"\"\"Compute n-gram based rouge scores.\n",
        "\n",
        "  Args:\n",
        "    target_ngrams: A Counter object mapping each ngram to number of\n",
        "      occurrences for the target text.\n",
        "    prediction_ngrams: A Counter object mapping each ngram to number of\n",
        "      occurrences for the prediction text.\n",
        "  Returns:\n",
        "    A Score object containing computed scores.\n",
        "  \"\"\"\n",
        "\n",
        "  intersection_ngrams_count = 0\n",
        "  for ngram in six.iterkeys(target_ngrams):\n",
        "    intersection_ngrams_count += min(target_ngrams[ngram],\n",
        "                                     prediction_ngrams[ngram])\n",
        "  target_ngrams_count = sum(target_ngrams.values())\n",
        "  prediction_ngrams_count = sum(prediction_ngrams.values())\n",
        "\n",
        "  precision = intersection_ngrams_count / max(prediction_ngrams_count, 1)\n",
        "  recall = intersection_ngrams_count / max(target_ngrams_count, 1)\n",
        "  fmeasure = scoring.fmeasure(precision, recall)\n",
        "\n",
        "  return scoring.Score(precision=precision, recall=recall, fmeasure=fmeasure)"
      ],
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.7/dist-packages (0.0.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (0.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.19.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge-score) (3.2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVczTefjQspt"
      },
      "source": [
        "## 来进行数据验证"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDzckvmMRGrW"
      },
      "source": [
        "def tokenize(text, stemmer):\n",
        "  \"\"\"Tokenize input text into a list of tokens.\n",
        "\n",
        "  This approach aims to replicate the approach taken by Chin-Yew Lin in\n",
        "  the original ROUGE implementation.\n",
        "\n",
        "  Args:\n",
        "    text: A text blob to tokenize.\n",
        "    stemmer: An optional stemmer.\n",
        "\n",
        "  Returns:\n",
        "    A list of string tokens extracted from input text.\n",
        "  \"\"\"\n",
        "\n",
        "  # Convert everything to lowercase.\n",
        "  text = text.lower()\n",
        "  # Replace any non-alpha-numeric characters with spaces.\n",
        "  # text = re.sub(r\"[^a-z0-9]+\", \" \", six.ensure_str(text))\n",
        "  # ^ [ / u4E00 - / u9FFF]+$\n",
        "  tokens = re.split(r\"\\s+\", text)\n",
        "\n",
        "  # print(tokens)\n",
        "  # print(1)\n",
        "  if stemmer:\n",
        "    # Only stem words more than 3 characters long.\n",
        "    tokens = [stemmer.stem(x) if len(x) > 3 else x for x in tokens]\n",
        "\n",
        "  # print(tokens)\n",
        "  # print(2)\n",
        "  # print(six.ensure_str(x))\n",
        "  # One final check to drop any empty or invalid tokens.\n",
        "  # tokens = [x for x in tokens if re.match(r\"^[/u4E00 - /u9FFF]+$\", six.ensure_str(x))]\n",
        "  # tokens = [x for x in tokens if re.match(r\"^[a-z0-9]+$\", six.ensure_str(x))]\n",
        "  tokens=[x for x in tokens if six.ensure_str]\n",
        "\n",
        "\n",
        "  # print(tokens)\n",
        "  # print(3)\n",
        "  return tokens"
      ],
      "execution_count": 268,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmLiVZtCRYnL",
        "outputId": "6daa1e6f-f528-4506-cb0f-d21c5f19d43a"
      },
      "source": [
        "test_eng_texts = [pair[0] for pair in train_pairs]\n",
        "for _ in range(1):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    translated = decode_sequence(input_sentence)\n",
        "    print(translated)"
      ],
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[start] 中餐厅 生活服务场所 临街院门 公司 美容美发店 住宅区 停车场 医药保健销售店 娱乐场所 彩票彩券销售点\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbbKuipqRdQy"
      },
      "source": [
        "# train_"
      ],
      "execution_count": 270,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mE-tLjztRJ5b"
      },
      "source": [
        "A=[]\n",
        "B=[]\n",
        "C=[]\n",
        "scorer = RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "for seq_index in range(0,300):\n",
        "  # translated = decode_sequence(train_pairs[seq_index][0])\n",
        "  scores =  scorer.score(train_pairs[seq_index][1].replace('[start]',\"\").replace('[end]','').strip(),\n",
        "  decode_sequence(train_pairs[seq_index][0]).replace('[start]',\"\").replace('[end]','').strip())\n",
        "  # print(scores['rouge1'])\n",
        "  tmp=scores['rouge1']\n",
        "  A.append(tmp[0])\n",
        "  B.append(tmp[1])\n",
        "  C.append(tmp[2])"
      ],
      "execution_count": 271,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNGzLVJBt5lk"
      },
      "source": [
        "## 查看数据得分"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMBaAUVjsXvy",
        "outputId": "5b216690-9c4e-4668-86bd-6439d6a4a883"
      },
      "source": [
        "scores"
      ],
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge1': Score(precision=0.4, recall=0.4, fmeasure=0.4000000000000001),\n",
              " 'rougeL': Score(precision=0.3, recall=0.3, fmeasure=0.3)}"
            ]
          },
          "metadata": {},
          "execution_count": 272
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "zLD0EJmGnlH5",
        "outputId": "5b14628b-b139-4b98-9912-3c10e9410af7"
      },
      "source": [
        "decode_sequence(train_pairs[1][0]).replace('[start]',\"\").strip()"
      ],
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'停车场 临街院门 生活服务场所 住宅区 美容美发店 中餐厅 洗浴推拿场所 医药保健销售店 物流速递 诊所'"
            ]
          },
          "metadata": {},
          "execution_count": 273
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYZPt2b9RKBb",
        "outputId": "4f8f97c5-c5fa-454a-aedd-451950f382f3"
      },
      "source": [
        "import numpy as np\n",
        "print(np.mean(A))\n",
        "print(np.mean(B))\n",
        "print(np.mean(C))"
      ],
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.495\n",
            "0.4925555555555556\n",
            "0.493462962962963\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVlisOCzm9js"
      },
      "source": [
        ""
      ],
      "execution_count": 274,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uam9VvMBXIkP"
      },
      "source": [
        "After 30 epochs, we get results such as:\n",
        "\n",
        "> She handed him the money.\n",
        "> [start] ella le pasó el dinero [end]\n",
        "\n",
        "> Tom has never heard Mary sing.\n",
        "> [start] tom nunca ha oído cantar a mary [end]\n",
        "\n",
        "> Perhaps she will come tomorrow.\n",
        "> [start] tal vez ella vendrá mañana [end]\n",
        "\n",
        "> I love to write.\n",
        "> [start] me encanta escribir [end]\n",
        "\n",
        "> His French is improving little by little.\n",
        "> [start] su francés va a [UNK] sólo un poco [end]\n",
        "\n",
        "> My hotel told me to call you.\n",
        "> [start] mi hotel me dijo que te [UNK] [end]"
      ]
    }
  ]
}